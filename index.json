[{"categories":null,"contents":"하루 일지 요약 기상 / 숙면 시간 새벽 1시에 잠들었다. 7시 반에 일어났다. 업무 및 공부 .. KCD 세미나를 다녀왔는데 두 세션 정도가 기억에 남는다. KT Cloud의 OVN으로 오픈스택과 쿠버네티스 네트워크 레이어를 통합한 내용과 현대차그룹의 ClusterAPI를 커스텀 하여 OpenStack 과 Baremetal에 함께 K8s를 배포하는 방법에 대한 세션이었다. OVN은 전날 밤에 모종의 이유로 열심히 개념을 공부했는데, 언젠가 실제로 Kube-OVN을 사용해보는 실습을 하고 싶다. 더 관심이 생긴 부분은 Open vSwitch와 openflow에 대한 부분이다. 문서라도 읽어봐야하나? ClusterAPI는 오픈스택과 Baremetal에 K8s를 배포하는 방법을 설명했는데, 이 세션도 내게 오퍼레이터 패턴에 실제로 익숙해져야 한다는 자극을 주었다. gpt가 준 토이 프로젝트 아이디어 중 하나는 자체 인증서의 자동 갱신 프로젝트였다. 도전해보려 한다. 세션이 인상 깊은 것은 아니었는데 Helm Chart 관련 발표를 진행하신 삼성 SDS 쪽의 연사 분은 인상이 참 좋으셨다. 나쁜 발표로 판단된 몇몇 세션도 있었다. 반면교사로 삼아야겠다. 기술의 더 심연을 계속 파고들어야 한다고 생각한다. 특히 기초적인 부분에서! 유데미 강의를 더 자주 빠르게 듣자 FRR을 통한 네트워크 실습을 방법을 찾아보자. 라우터 설정 실습을 위한 커스텀 리소스 등을 아예 오퍼레이터 패턴으로 확장할 수 도 있지 않을까? 이외의 생각 5월이 빠르게 지나가고 있다. 세미나 장소의 의자가 너무 불편했다. 집으로 돌아오는 길에, 1호선에 너무 시달리고, 또 지하철 안에서 너무 고통받기도 했다. 앉아 있는 지금도 발바닥이 쑤시다. 목이 타기도 하는데, 이건 저녁을 짜게 먹어서 그런 것 같다. 세미나를 보며, 작은 규모의 모임이더라도 뭔가 나도 발표해보고 싶은 생각이 들었다. 발표 할 게 없어서 그렇지만서도. 집와서 어어 하다보니 벌써 9시다. 오늘 추가 공부는 못할 것 같다. ","date":"May 22, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/100.-diary/eng/2025-05-22/","summary":"\u003ch3 id=\"하루-일지-요약\"\u003e하루 일지 요약\u003c/h3\u003e\n\u003ch4 id=\"기상--숙면-시간\"\u003e기상 / 숙면 시간\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e새벽 1시에 잠들었다.\u003c/li\u003e\n\u003cli\u003e7시 반에 일어났다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"업무-및-공부-\"\u003e업무 및 공부 ..\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eKCD 세미나를 다녀왔는데 두 세션 정도가 기억에 남는다.\n\u003cul\u003e\n\u003cli\u003eKT Cloud의 OVN으로 오픈스택과 쿠버네티스 네트워크 레이어를 통합한 내용과 현대차그룹의 ClusterAPI를 커스텀 하여 OpenStack 과 Baremetal에 함께 K8s를 배포하는 방법에 대한 세션이었다.\u003c/li\u003e\n\u003cli\u003eOVN은 전날 밤에 모종의 이유로 열심히 개념을 공부했는데, 언젠가 실제로 Kube-OVN을 사용해보는 실습을 하고 싶다.\n\u003cul\u003e\n\u003cli\u003e더 관심이 생긴 부분은 Open vSwitch와 openflow에 대한 부분이다. 문서라도 읽어봐야하나?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eClusterAPI는 오픈스택과 Baremetal에 K8s를 배포하는 방법을 설명했는데, 이 세션도 내게 오퍼레이터 패턴에 실제로 익숙해져야 한다는 자극을 주었다. gpt가 준 토이 프로젝트 아이디어 중 하나는 자체 인증서의 자동 갱신 프로젝트였다. 도전해보려 한다.\u003c/li\u003e\n\u003cli\u003e세션이 인상 깊은 것은 아니었는데 Helm Chart 관련 발표를 진행하신 삼성 SDS 쪽의 연사 분은 인상이 참 좋으셨다.\u003c/li\u003e\n\u003cli\u003e나쁜 발표로 판단된 몇몇 세션도 있었다. 반면교사로 삼아야겠다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e기술의 더 심연을 계속 파고들어야 한다고 생각한다. 특히 기초적인 부분에서!\n\u003cul\u003e\n\u003cli\u003e유데미 강의를 더 자주 빠르게 듣자\u003c/li\u003e\n\u003cli\u003eFRR을 통한 네트워크 실습을 방법을 찾아보자.\n\u003cul\u003e\n\u003cli\u003e라우터 설정 실습을 위한 커스텀 리소스 등을 아예 오퍼레이터 패턴으로 확장할 수 도 있지 않을까?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"이외의-생각\"\u003e이외의 생각\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e5월이 빠르게 지나가고 있다.\u003c/li\u003e\n\u003cli\u003e세미나 장소의 의자가 너무 불편했다. 집으로 돌아오는 길에, 1호선에 너무 시달리고, 또 지하철 안에서 너무 고통받기도 했다. 앉아 있는 지금도 발바닥이 쑤시다. 목이 타기도 하는데, 이건 저녁을 짜게 먹어서 그런 것 같다.\u003c/li\u003e\n\u003cli\u003e세미나를 보며, 작은 규모의 모임이더라도 뭔가 나도 발표해보고 싶은 생각이 들었다. 발표 할 게 없어서 그렇지만서도.\u003c/li\u003e\n\u003cli\u003e집와서 어어 하다보니 벌써 9시다. 오늘 추가 공부는 못할 것 같다.\u003c/li\u003e\n\u003c/ul\u003e","tags":null,"title":"2025-05-22"},{"categories":null,"contents":"하루 일지 요약 기상 / 숙면 시간 어제는 11시 20분에 잠들었다. 오늘은 6시 40분에 일어났다. 일어나서 뉴스를 틀었다. 업무 및 공부 .. 데이터센터 네트워크 책 진행을 못했다. 대신 대규모 데이터 센터에서 오픈스택 클러스터를 어떤 식으로 구성해야 하는지에 대해 고민했다.( gpt와 대화하면서 \u0026hellip; 신뢰성은 모르겠다 ) 컨트롤러 / 네트워크 / 컴퓨트 / 스토리지 영역을 나누는 것은 기본이다. 랙 4대에 각 서버 10개씩 넣는다고 가정했다. 각 랙 상단에 2~3개의 리프 스위치를 둔다고 생각했다. 각각 퍼블릭, 관리, 스토리지 용도이며, 이더넷 망에서 VLAN 혹은 VRF로 나눌 수 있다. 다만, 실전 경험이 없기 때문에 정말 이런 식으로 구성해야 하는지는 잘 모르겠다. 스파인 스위치와 리프 스위치 연결은 25G로 구성하고, 리프 스위치와 서버 연결은 10G로 구성한다. 스파인 스위치는 경계 스위치(Border Switch)와 또 연결되어 있으며, Border는 이중화된 라우터(R1/R2)와 연결된다. 이중화된 라우터는 방화벽, 로드밸런서, 외부 BGP 피어 등을 바라보고 있으며, 결국 외부와의 통신 관점에서 네트워크 출입구 역할을 한다. 컨트롤러 서버는 HA 구성을 위해 3대로 구성한다. 여기에는 API, 데이터베이스, 메시지 큐, 스케줄러, OVN-Northd 등을 배치할 수 있다. 컴퓨트는 총 20대로 구성하고, 각각 듀얼 소켓 기반 CPU (56vCPU 수준)와 256~384GB RAM 정도를 가정했다. Overcommit 비율은 CPU 2:1, RAM 1.2:1 정도로 잡으면 c2m4 스펙(2vCPU / 4GB RAM) 기준으로 대당 약 50~60대 정도의 VM을 안정적으로 배포할 수 있다. 전체 Compute 용량 기준으로는 약 1,120대의 VM을 수용할 수 있다. 스토리지는 Ceph를 가정했고, 10대의 OSD 서버에 12TB HDD 12개씩 구성했다. 이를 기준으로 총 raw 용량은 1,440TB이며, 3중 복제를 기준으로 usable 용량은 약 480TB 정도가 된다. 이를 기반으로 하면 VM당 평균 약 400GB 가량의 블록 스토리지(RBD)를 할당할 수 있고, 운영 시에는 스냅샷, 백업 등을 고려하여 300~350GB 수준으로 관리할 수 있을 것이다. Ceph OSD 서버는 16~24코어, 128~192GB RAM을 가진 중급 이상의 스토리지 노드로 설정하며, 1 OSD당 약 1.0~1.5 vCPU, 4~6GB RAM이 필요하다는 Ceph 가이드라인을 참고했다. 따라서 서버당 12개 디스크 구성이라면 12 OSD 기준으로 16코어는 가능한 하한이며, 여유를 두고 설계한다면 24코어가 더 적절할 수 있다. 나머지 7대는 네트워크/인프라 영역으로 분류되며, 모니터링, 로깅, 메트릭 수집, 배스천 서버, CI/CD 엔진, RadosGW, 백업 노드 등으로 활용할 수 있다. 실제 운영 시에는 이 노드들이 가볍게 보이지만, 지속적인 로깅, 리텐션, 메트릭 수집이 클러스터 전체 성능에 영향을 주기 때문에 독립 배치가 바람직하다. OVN 사용을 기본으로 생각했고, NS 트래픽은 BGP/EVPN 등을 고려하고, EW 트래픽은 Geneve를 고려했다. EW 쪽에서 SR-IOV/DPDK 도 옵션이라 하는데, 조금 난이도가 있게 느껴진다. 그래서 이런 구성에서 실제 스위치, 라우터 설정을 어떻게 해야하는 것일까 \u0026hellip; 라는 어려운 숙제를 가지고 있다. 내일은 KCD 2025에 참석하기 위해서 대강 8시 30분에 출발해야 한다. 이외의 생각 더워진다. 저녁 먹을 때 너무 덥고 꿉꿉해서 선풍기를 꺼냈다. 아직 청소가 안 된거라 찝찝하긴 했는데 더운 건 참을 수 없었다. 밥 먹을 때 보통 윈도우 컴퓨터로 유튜브를 보는데, 오늘은 무슨 날인지 컴퓨터가 갑자기 죽어버렸다. 허탈한 마음으로 본체 분리해서 전원선 뽑고 다른 곳 가서 잠시 기다렸다가 다시 연결하니 잘 켜졌다. 이게 뭘까 \u0026hellip; 전원 공급이 불안정한건지, 아니면 다른 문제인지 모르겠다. 컴퓨터 분리한 김에 청소도 하고, 구조도 바꿨다. 책상 위의 메인 컴퓨터를 옛날 맥북으로 바꿨다. 이제 좀 더 공부에 집중할 수 있는 환경이 되지 않았을까 \u0026hellip; ","date":"May 21, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/100.-diary/eng/2025-05-21/","summary":"\u003ch3 id=\"하루-일지-요약\"\u003e하루 일지 요약\u003c/h3\u003e\n\u003ch4 id=\"기상--숙면-시간\"\u003e기상 / 숙면 시간\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e어제는 11시 20분에 잠들었다.\u003c/li\u003e\n\u003cli\u003e오늘은 6시 40분에 일어났다. 일어나서 뉴스를 틀었다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"업무-및-공부-\"\u003e업무 및 공부 ..\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e데이터센터 네트워크 책 진행을 못했다.\u003c/li\u003e\n\u003cli\u003e대신 대규모 데이터 센터에서 오픈스택 클러스터를 어떤 식으로 구성해야 하는지에 대해 고민했다.( gpt와 대화하면서 \u0026hellip; 신뢰성은 모르겠다 )\n\u003cul\u003e\n\u003cli\u003e컨트롤러 / 네트워크 / 컴퓨트 / 스토리지 영역을 나누는 것은 기본이다.\u003c/li\u003e\n\u003cli\u003e랙 4대에 각 서버 10개씩 넣는다고 가정했다.\u003c/li\u003e\n\u003cli\u003e각 랙 상단에 2~3개의 리프 스위치를 둔다고 생각했다. 각각 퍼블릭, 관리, 스토리지 용도이며, 이더넷 망에서 VLAN 혹은 VRF로 나눌 수 있다. 다만, 실전 경험이 없기 때문에 정말 이런 식으로 구성해야 하는지는 잘 모르겠다.\u003c/li\u003e\n\u003cli\u003e스파인 스위치와 리프 스위치 연결은 25G로 구성하고, 리프 스위치와 서버 연결은 10G로 구성한다. 스파인 스위치는 경계 스위치(Border Switch)와 또 연결되어 있으며, Border는 이중화된 라우터(R1/R2)와 연결된다.\u003c/li\u003e\n\u003cli\u003e이중화된 라우터는 방화벽, 로드밸런서, 외부 BGP 피어 등을 바라보고 있으며, 결국 외부와의 통신 관점에서 네트워크 출입구 역할을 한다.\u003c/li\u003e\n\u003cli\u003e컨트롤러 서버는 HA 구성을 위해 3대로 구성한다. 여기에는 API, 데이터베이스, 메시지 큐, 스케줄러, OVN-Northd 등을 배치할 수 있다.\u003c/li\u003e\n\u003cli\u003e컴퓨트는 총 20대로 구성하고, 각각 듀얼 소켓 기반 CPU (56vCPU 수준)와 256~384GB RAM 정도를 가정했다. Overcommit 비율은 CPU 2:1, RAM 1.2:1 정도로 잡으면 c2m4 스펙(2vCPU / 4GB RAM) 기준으로 대당 약 50~60대 정도의 VM을 안정적으로 배포할 수 있다. 전체 Compute 용량 기준으로는 약 1,120대의 VM을 수용할 수 있다.\u003c/li\u003e\n\u003cli\u003e스토리지는 Ceph를 가정했고, 10대의 OSD 서버에 12TB HDD 12개씩 구성했다. 이를 기준으로 총 raw 용량은 1,440TB이며, 3중 복제를 기준으로 usable 용량은 약 480TB 정도가 된다. 이를 기반으로 하면 VM당 평균 약 400GB 가량의 블록 스토리지(RBD)를 할당할 수 있고, 운영 시에는 스냅샷, 백업 등을 고려하여 300~350GB 수준으로 관리할 수 있을 것이다.\u003c/li\u003e\n\u003cli\u003eCeph OSD 서버는 16~24코어, 128~192GB RAM을 가진 중급 이상의 스토리지 노드로 설정하며, 1 OSD당 약 1.0~1.5 vCPU, 4~6GB RAM이 필요하다는 Ceph 가이드라인을 참고했다. 따라서 서버당 12개 디스크 구성이라면 12 OSD 기준으로 16코어는 가능한 하한이며, 여유를 두고 설계한다면 24코어가 더 적절할 수 있다.\u003c/li\u003e\n\u003cli\u003e나머지 7대는 네트워크/인프라 영역으로 분류되며, 모니터링, 로깅, 메트릭 수집, 배스천 서버, CI/CD 엔진, RadosGW, 백업 노드 등으로 활용할 수 있다. 실제 운영 시에는 이 노드들이 가볍게 보이지만, 지속적인 로깅, 리텐션, 메트릭 수집이 클러스터 전체 성능에 영향을 주기 때문에 독립 배치가 바람직하다.\u003c/li\u003e\n\u003cli\u003eOVN 사용을 기본으로 생각했고, NS 트래픽은 BGP/EVPN 등을 고려하고, EW 트래픽은 Geneve를 고려했다. EW 쪽에서 SR-IOV/DPDK 도 옵션이라 하는데, 조금 난이도가 있게 느껴진다.\u003c/li\u003e\n\u003cli\u003e그래서 이런 구성에서 실제 스위치, 라우터 설정을 어떻게 해야하는 것일까 \u0026hellip; 라는 어려운 숙제를 가지고 있다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e내일은 KCD 2025에 참석하기 위해서 대강 8시 30분에 출발해야 한다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"이외의-생각\"\u003e이외의 생각\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e더워진다. 저녁 먹을 때 너무 덥고 꿉꿉해서 선풍기를 꺼냈다. 아직 청소가 안 된거라 찝찝하긴 했는데 더운 건 참을 수 없었다.\u003c/li\u003e\n\u003cli\u003e밥 먹을 때 보통 윈도우 컴퓨터로 유튜브를 보는데, 오늘은 무슨 날인지 컴퓨터가 갑자기 죽어버렸다. 허탈한 마음으로 본체 분리해서 전원선 뽑고 다른 곳 가서 잠시 기다렸다가 다시 연결하니 잘 켜졌다. 이게 뭘까 \u0026hellip; 전원 공급이 불안정한건지, 아니면 다른 문제인지 모르겠다.\u003c/li\u003e\n\u003cli\u003e컴퓨터 분리한 김에 청소도 하고, 구조도 바꿨다. 책상 위의 메인 컴퓨터를 옛날 맥북으로 바꿨다. 이제 좀 더 공부에 집중할 수 있는 환경이 되지 않았을까 \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e","tags":null,"title":"2025-05-21"},{"categories":null,"contents":"하루 일지 요약 기상 / 숙면 시간 어제는 11시 30분에 잠들었다. 오늘은 7시에 일어났다. 눈은 6시에 뜨긴 했었는데 더 자고 싶었다. (가짜 졸림 아닐까?) 업무 및 공부 .. 솔루션 상태 검증 스크립트를 가이드와 함께 제공 해야한다. 이상한 요구사항과 함께 \u0026hellip; 데이터센터 네트워크 책 10.9.1 까지 읽었다. 멀티캐스트 라우팅 파트가 너무 어려웠다. 그리고 약간은 디테일이 부족한 것 같다고 느낀다. 이론 보강과 실습이 추가되어야 할 것 같다. 운영체제 책을 중간 중간 끼어서 읽어야 하는데 너무 소홀하다. 대신 오늘은 유데미 강의를 산책하면서 들었다. 영어 리스닝 연습 겸으로. 뭔가 팟캐스트 듣는 느낌이라 좋은 것 같다. 이외의 생각 주말과 월요일 일지가 없는 것은, 집에서 노트북을 열지 않았다는 뜻과 같다. 주말에는 열심히 놀았다. 스타워즈 드라마 안도르 시즌2를 정주행 했다. 로그원과 안도르의 매력은 스타워즈 세계관을 잘 살리면서도, 그 세계관에 얽매이지 않는 점이다. 나는 작품만의 내러티브를 가지고 있다는 점을 아주 높게 산다. 특히 시즌 1과 2 모두 \u0026ldquo;민중의 봉기\u0026quot;를 표현하였는데, 두 장면의 설정상 독립성과 연관성이 매력적이었다. 어제 저녁은 오랜만에 직접 요리를 했다. 그 때문인지 뭔가 별로 한 것도 없는 것 같았는데 시간이 많이 흘렀었다. 회사에서 있는 시간이 줄고, 좀 더 나만의 시간을 가졌으면 좋겠다는 생각이다. 다시 몸이 무거워지고 살이 찌는 것 같아서 위기감을 느끼고 러닝을 시도했다. 역시 오랜만에 뛰니까 별로 못뛰었다. 대신 걸었다. 집에서 안양천을 따라 회사 방향으로 왕복하면 대강 5km 정도 된다. 벌레가 많아서 마스크를 써야하나 고민이다. 내일도 오늘과 같을 수 있을지 \u0026hellip; 하루가 짧다. 괴롭게 짧다. 성취감이 없다. ","date":"May 20, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/100.-diary/eng/2025-05-20/","summary":"\u003ch3 id=\"하루-일지-요약\"\u003e하루 일지 요약\u003c/h3\u003e\n\u003ch4 id=\"기상--숙면-시간\"\u003e기상 / 숙면 시간\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e어제는 11시 30분에 잠들었다.\u003c/li\u003e\n\u003cli\u003e오늘은 7시에 일어났다. 눈은 6시에 뜨긴 했었는데 더 자고 싶었다. (가짜 졸림 아닐까?)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"업무-및-공부-\"\u003e업무 및 공부 ..\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e솔루션 상태 검증 스크립트를 가이드와 함께 제공 해야한다. 이상한 요구사항과 함께 \u0026hellip;\u003c/li\u003e\n\u003cli\u003e데이터센터 네트워크 책 10.9.1 까지 읽었다. 멀티캐스트 라우팅 파트가 너무 어려웠다. 그리고 약간은 디테일이 부족한 것 같다고 느낀다. 이론 보강과 실습이 추가되어야 할 것 같다.\u003c/li\u003e\n\u003cli\u003e운영체제 책을 중간 중간 끼어서 읽어야 하는데 너무 소홀하다. 대신 오늘은 유데미 강의를 산책하면서 들었다. 영어 리스닝 연습 겸으로. 뭔가 팟캐스트 듣는 느낌이라 좋은 것 같다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"이외의-생각\"\u003e이외의 생각\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e주말과 월요일 일지가 없는 것은, 집에서 노트북을 열지 않았다는 뜻과 같다.\u003c/li\u003e\n\u003cli\u003e주말에는 열심히 놀았다. 스타워즈 드라마 안도르 시즌2를 정주행 했다. 로그원과 안도르의 매력은 스타워즈 세계관을 잘 살리면서도, 그 세계관에 얽매이지 않는 점이다. 나는 작품만의 내러티브를 가지고 있다는 점을 아주 높게 산다. 특히 시즌 1과 2 모두 \u0026ldquo;민중의 봉기\u0026quot;를 표현하였는데, 두 장면의 설정상 독립성과 연관성이 매력적이었다.\u003c/li\u003e\n\u003cli\u003e어제 저녁은 오랜만에 직접 요리를 했다. 그 때문인지 뭔가 별로 한 것도 없는 것 같았는데 시간이 많이 흘렀었다. 회사에서 있는 시간이 줄고, 좀 더 나만의 시간을 가졌으면 좋겠다는 생각이다.\u003c/li\u003e\n\u003cli\u003e다시 몸이 무거워지고 살이 찌는 것 같아서 위기감을 느끼고 러닝을 시도했다. 역시 오랜만에 뛰니까 별로 못뛰었다. 대신 걸었다. 집에서 안양천을 따라 회사 방향으로 왕복하면 대강 5km 정도 된다. 벌레가 많아서 마스크를 써야하나 고민이다. 내일도 오늘과 같을 수 있을지 \u0026hellip;\u003c/li\u003e\n\u003cli\u003e하루가 짧다. 괴롭게 짧다. 성취감이 없다.\u003c/li\u003e\n\u003c/ul\u003e","tags":null,"title":"2025-05-20"},{"categories":null,"contents":"하루 일지 요약 기상 / 숙면 시간 어제는 술먹고 1시에 잠들었다. 오늘은 7시 35분에 일어났다. 숙취에 괴로워하며 \u0026hellip; 업무 및 공부 .. 앞으로 어떤 업무를 할 수 있는지(혹은 하고 싶은지) 요청하셔서, 정리해서 올려드렸다. 내 마음은 플랫폼 엔지니어링에 가까운 거 같다. 책 읽는 게 즐겁다. 데이터센터 네트워크 책 6.8 까지 읽었다. 하지만 집에 오면 책을 펴는 습관이 잘 생기지 않는다. 환경의 어떤 부분이 문제인 걸까 고민 중이다. 이외의 생각 술은 안 먹는게 좋다. 밀린 집 청소가 필요하다. 밤에 \u0026ldquo;승부\u0026quot;라는 영화를 봤다. 이창호와 조훈현의 바둑 대결을 다룬 영화다. 예전에 유튜브에서 관련 다큐를 본 적이 있다. 그 영상에서 느껴졌던 사제 간의 불편한 기류가 영화에서도 잘 표현되었다. 특히 이병헌이 연기한 디테일이 대단했다. 영화 중반 이후에는 정말 조훈현을 보는 것 같았다. 영화의 미장센도 좋았다. 다만, \u0026ldquo;승부\u0026quot;라는 단어로 숨겨진 의미가 무엇일까? 승부는 단순히 이기고 지는 것인가? 이기고 지는 것에 대한 집착이 과연 바람직한 것인가? 세상은 너무 1등만 기억한다. 요즈음, 더 나은 미래를 고민하는 나로서는 최고는 커녕 그 문턱조차 넘기지 못하는 현실에 고통스러워 하고 있다. 영화에선 결국 조훈현이 다시 타이틀을 하나 따내는 것으로 마무리 되지만, 그 장면이 지금에 나에겐 크게 위로가 되지 않았다. 최고들의 이야기에선 가르침이나 위로를 얻기 힘들다는 걸 느꼈다. 오히려 수 프레임만을 차지하며 바둑 해설을 경청하는 무수한 관중들에게 내 모습을 보는 느낌이었다. 그럼에도, 바둑은 자신과의 싸움 이라는 글귀 자체는 내게 경종을 울리긴 하였다. 오랜만에 다시 바둑을 두고 싶다는 생각이 들었다. 근데 생각만 들었다. 나한테는 차라리 체스가 어울리는 거 같다. ","date":"May 16, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/100.-diary/eng/2025-05-16/","summary":"\u003ch3 id=\"하루-일지-요약\"\u003e하루 일지 요약\u003c/h3\u003e\n\u003ch4 id=\"기상--숙면-시간\"\u003e기상 / 숙면 시간\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e어제는 술먹고 1시에 잠들었다.\u003c/li\u003e\n\u003cli\u003e오늘은 7시 35분에 일어났다. 숙취에 괴로워하며 \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"업무-및-공부-\"\u003e업무 및 공부 ..\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e앞으로 어떤 업무를 할 수 있는지(혹은 하고 싶은지) 요청하셔서, 정리해서 올려드렸다. 내 마음은 플랫폼 엔지니어링에 가까운 거 같다.\u003c/li\u003e\n\u003cli\u003e책 읽는 게 즐겁다. 데이터센터 네트워크 책 6.8 까지 읽었다.\u003c/li\u003e\n\u003cli\u003e하지만 집에 오면 책을 펴는 습관이 잘 생기지 않는다. 환경의 어떤 부분이 문제인 걸까 고민 중이다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"이외의-생각\"\u003e이외의 생각\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e술은 안 먹는게 좋다.\u003c/li\u003e\n\u003cli\u003e밀린 집 청소가 필요하다.\u003c/li\u003e\n\u003cli\u003e밤에 \u0026ldquo;승부\u0026quot;라는 영화를 봤다. 이창호와 조훈현의 바둑 대결을 다룬 영화다. 예전에 유튜브에서 관련 다큐를 본 적이 있다. 그 영상에서 느껴졌던 사제 간의 불편한 기류가 영화에서도 잘 표현되었다. 특히 이병헌이 연기한 디테일이 대단했다. 영화 중반 이후에는 정말 조훈현을 보는 것 같았다. 영화의 미장센도 좋았다.\n\u003cul\u003e\n\u003cli\u003e다만, \u0026ldquo;승부\u0026quot;라는 단어로 숨겨진 의미가 무엇일까? 승부는 단순히 이기고 지는 것인가? 이기고 지는 것에 대한 집착이 과연 바람직한 것인가? 세상은 너무 1등만 기억한다.\u003c/li\u003e\n\u003cli\u003e요즈음, 더 나은 미래를 고민하는 나로서는 최고는 커녕 그 문턱조차 넘기지 못하는 현실에 고통스러워 하고 있다. 영화에선 결국 조훈현이 다시 타이틀을 하나 따내는 것으로 마무리 되지만, 그 장면이 지금에 나에겐 크게 위로가 되지 않았다. 최고들의 이야기에선 가르침이나 위로를 얻기 힘들다는 걸 느꼈다. 오히려 수 프레임만을 차지하며 바둑 해설을 경청하는 무수한 관중들에게 내 모습을 보는 느낌이었다.\u003c/li\u003e\n\u003cli\u003e그럼에도, 바둑은 \u003cstrong\u003e자신과의 싸움\u003c/strong\u003e 이라는 글귀 자체는 내게 경종을 울리긴 하였다.\u003c/li\u003e\n\u003cli\u003e오랜만에 다시 바둑을 두고 싶다는 생각이 들었다. 근데 생각만 들었다. 나한테는 차라리 체스가 어울리는 거 같다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","tags":null,"title":"2025-05-16"},{"categories":null,"contents":"","date":"May 15, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/500.-tech_dev/%EA%B8%B0%ED%83%80/%EA%B8%B0%EC%88%A0-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EC%9C%A0%EC%A7%80%EB%B3%B4%EC%88%98/","summary":"","tags":null,"title":"방치해두었던 hugo 블로그 최신화 경험"},{"categories":null,"contents":"하루 일지 요약 기상 / 숙면 시간 어제는 12시 반쯤 잠들었다. 오늘은 7시 35분에 일어났다. 업무 및 공부 .. 오픈 스택 관련 글을 더 많이 써봐야겠다는 생각을 했다.(https://www.linkedin.com/posts/daewon-kim-80b1076b_%EC%B5%9C%EA%B7%BC%EC%97%90-%EC%A2%80-%EC%A7%88%EB%AC%B8%EC%9D%84-%EB%A7%8E%EC%9D%B4-%EC%A3%BC%EC%85%A8%EC%8A%B5%EB%8B%88%EB%8B%A4-%EC%98%A4%ED%94%88%EC%8A%A4%ED%83%9D-%EA%B5%90%EC%9C%A1%EC%9E%90%EB%A3%8C%EA%B0%80-%EC%9E%88%EB%82%98%EC%9A%94-activity-7328298813931061249-XDuS?utm_source=share\u0026utm_medium=member_desktop\u0026rcm=ACoAAEQdiIYBKymviVGUosvzr4p4D_brz1ikTyg) 클라우드 네이티브를 위한 데이터센터 네트워크 구축 5장을 읽었다. 라우팅 관련으로 BGP와 OSPF에 대한 내용을 다루고 있다. (구체적인 이론적 내용은 아니었지만) 간단한 특징이 서술되었고, 데이터센터에서 어떤 라우팅 방식을 선택해야하는 지에 대한 설명이 있었다. 결론적으로 BGP + EVPN 구성이 매력적인 것 같은데, 오픈스택 관점에서 OVN과 함께 동시 EVPN 활용이 가능한건가 궁금해졌다. gpt와의 대담에서는 아리쏭한 결과만 남았다. 6장이 네트워크 가상화 파트인데, 이 부분에서 약간의 해답이 나오지 않을까 기대 중이다. 이외의 생각 어두컴컴했어서 그런지 눈을 떴는데 원래 8시 출근 기준으로 일어나던 시간을 조금 넘긴 시간이었다. 국방의 의무에서 벗어난 몸이 된 기념으로 한 시간 더 잤다. 비가 와서 긴 팔을 입고 나갔다. ","date":"May 15, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/100.-diary/eng/2025-05-15/","summary":"\u003ch3 id=\"하루-일지-요약\"\u003e하루 일지 요약\u003c/h3\u003e\n\u003ch4 id=\"기상--숙면-시간\"\u003e기상 / 숙면 시간\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e어제는 12시 반쯤 잠들었다.\u003c/li\u003e\n\u003cli\u003e오늘은 7시 35분에 일어났다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"업무-및-공부-\"\u003e업무 및 공부 ..\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e오픈 스택 관련 글을 더 많이 써봐야겠다는 생각을 했다.(\u003ca href=\"https://www.linkedin.com/posts/daewon-kim-80b1076b_%EC%B5%9C%EA%B7%BC%EC%97%90-%EC%A2%80-%EC%A7%88%EB%AC%B8%EC%9D%84-%EB%A7%8E%EC%9D%B4-%EC%A3%BC%EC%85%A8%EC%8A%B5%EB%8B%88%EB%8B%A4-%EC%98%A4%ED%94%88%EC%8A%A4%ED%83%9D-%EA%B5%90%EC%9C%A1%EC%9E%90%EB%A3%8C%EA%B0%80-%EC%9E%88%EB%82%98%EC%9A%94-activity-7328298813931061249-XDuS?utm_source=share\u0026amp;utm_medium=member_desktop\u0026amp;rcm=ACoAAEQdiIYBKymviVGUosvzr4p4D_brz1ikTyg\" target=\"_blank\" rel=\"noopener\"\u003ehttps://www.linkedin.com/posts/daewon-kim-80b1076b_%EC%B5%9C%EA%B7%BC%EC%97%90-%EC%A2%80-%EC%A7%88%EB%AC%B8%EC%9D%84-%EB%A7%8E%EC%9D%B4-%EC%A3%BC%EC%85%A8%EC%8A%B5%EB%8B%88%EB%8B%A4-%EC%98%A4%ED%94%88%EC%8A%A4%ED%83%9D-%EA%B5%90%EC%9C%A1%EC%9E%90%EB%A3%8C%EA%B0%80-%EC%9E%88%EB%82%98%EC%9A%94-activity-7328298813931061249-XDuS?utm_source=share\u0026utm_medium=member_desktop\u0026rcm=ACoAAEQdiIYBKymviVGUosvzr4p4D_brz1ikTyg\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e클라우드 네이티브를 위한 데이터센터 네트워크 구축\u003c/strong\u003e 5장을 읽었다. 라우팅 관련으로 BGP와 OSPF에 대한 내용을 다루고 있다. (구체적인 이론적 내용은 아니었지만) 간단한 특징이 서술되었고, 데이터센터에서 어떤 라우팅 방식을 선택해야하는 지에 대한 설명이 있었다. 결론적으로 BGP + EVPN 구성이 매력적인 것 같은데, 오픈스택 관점에서 OVN과 함께 동시 EVPN 활용이 가능한건가 궁금해졌다. gpt와의 대담에서는 아리쏭한 결과만 남았다. 6장이 네트워크 가상화 파트인데, 이 부분에서 약간의 해답이 나오지 않을까 기대 중이다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"이외의-생각\"\u003e이외의 생각\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e어두컴컴했어서 그런지 눈을 떴는데 원래 8시 출근 기준으로 일어나던 시간을 조금 넘긴 시간이었다. 국방의 의무에서 벗어난 몸이 된 기념으로 한 시간 더 잤다.\u003c/li\u003e\n\u003cli\u003e비가 와서 긴 팔을 입고 나갔다.\u003c/li\u003e\n\u003c/ul\u003e","tags":null,"title":"2025-05-15"},{"categories":null,"contents":"하루 일지 요약 매일매일 일지를 써야겠다는 생각을 가지게 되었다. 또 작심삼일이 될련지는 모르겠지만, 그래도 해보자.\n기상 / 숙면 시간 어제는 21시 쯤에 잠들었다. 오늘은 6시 50분에 일어났다. 업무 및 공부 .. 클라우드 네이티브를 위한 데이터센터 네트워크 구축 이라는 책을 읽고 있다. 4장 까지 읽었다. 과거의 네트워크 구성, 클로스(리프-스파인) 구조, NOS, OpenFlow 등 과거 어디선가 들어봤던, 하지만 흩어져 있던 개념들이 한 곳에 모이는 느낌이다. 큰 역사의 흐름을 알게 되니, 현재의 기술들이 왜 그렇게 발전했는지 알 수 있었고 그래서 좀 더 이해가 잘 되는 것 같았다. 예를 들어, STP 라는게 왜 필요했고, 지금은 왜 L3 기반 구조로 가는지 배울 수 있었다. 아직 좀 헷갈리긴 하지만. 기능 개발에 대한 프로세스에 대한 생각을 해봤다. 최근에 기존에 이미 개발되었던 코드를 도식화 하는 작업을 수행했는데, 새로 만드는 기능 건에 대해서 모두가 구조적 도식화를 수행하는 게 필수적이라는 생각을 했다. 이미 유지보수에 어려움을 겪고 있는 상황에서 과거의 코드들을 도식화 하는 것은 아무래도 무리겠지만 말이다. 그리고 전체 솔루션의 구조적 변경에는 강력한 힘을 가진 아키텍쳐 혹은 메인테이너가 존재해야 하지 않을까도 생각했다. 5월의 절반이 지나가는 시점에서도 아직 업무가 구체화되지 않았다. 내가 하고 싶은 일이 정확히 뭔지 점점 더 아리쏭해진다. Cilium을 통한 노드간 iBGP 통신에 성공하고 싶다. 다만 오늘 그쪽에 신경 쓸 여력이 없었다. Cilium Up\u0026amp;Running 이라는 책이 만들어지고 있다는 정보를 습득했다. 공룡책(운영체제) 책도 진행하지 못했다. 어쩌다보니 드문드문 손이 가게된다. 습관적으로 조금씩 읽어나가야 할텐데 말이다. 이외의 생각 더워진다. 반팔옷을 입어야겠다. 마음을 다잡아야 겠다. 요즘 너무 심란한 상태에 있었다. 보는 방향을 바꿔야겠다. 운동을 해야한다. 퇴근길은 되도록이면 걸어가자. 다만, 걸으면서 들을만한 좋은 팟캐스트가 있으면 좋겠다. 글쓰기에는 꾸준함이 필요하다. 미니서버 3대를 구성하고 싶다. 오픈스택 용 컨트롤러 + 컴퓨트 + 스토리지 구성으로 \u0026hellip; ","date":"May 14, 2025","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/100.-diary/eng/2025-05-14/","summary":"\u003ch3 id=\"하루-일지-요약\"\u003e하루 일지 요약\u003c/h3\u003e\n\u003cp\u003e매일매일 일지를 써야겠다는 생각을 가지게 되었다.\n또 작심삼일이 될련지는 모르겠지만, 그래도 해보자.\u003c/p\u003e\n\u003ch4 id=\"기상--숙면-시간\"\u003e기상 / 숙면 시간\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e어제는 21시 쯤에 잠들었다.\u003c/li\u003e\n\u003cli\u003e오늘은 6시 50분에 일어났다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"업무-및-공부-\"\u003e업무 및 공부 ..\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e클라우드 네이티브를 위한 데이터센터 네트워크 구축\u003c/strong\u003e 이라는 책을 읽고 있다. 4장 까지 읽었다.\n과거의 네트워크 구성, 클로스(리프-스파인) 구조, NOS, OpenFlow 등 과거 어디선가 들어봤던, 하지만 흩어져 있던 개념들이 한 곳에 모이는 느낌이다.\n큰 역사의 흐름을 알게 되니, 현재의 기술들이 왜 그렇게 발전했는지 알 수 있었고 그래서 좀 더 이해가 잘 되는 것 같았다.\n예를 들어, STP 라는게 왜 필요했고, 지금은 왜 L3 기반 구조로 가는지 배울 수 있었다. 아직 좀 헷갈리긴 하지만.\u003c/li\u003e\n\u003cli\u003e기능 개발에 대한 프로세스에 대한 생각을 해봤다. 최근에 기존에 이미 개발되었던 코드를 도식화 하는 작업을 수행했는데,\n새로 만드는 기능 건에 대해서 모두가 구조적 도식화를 수행하는 게 필수적이라는 생각을 했다. 이미 유지보수에 어려움을 겪고 있는 상황에서 과거의 코드들을 도식화 하는 것은 아무래도 무리겠지만 말이다. 그리고 전체 솔루션의 구조적 변경에는 강력한 힘을 가진 아키텍쳐 혹은 메인테이너가 존재해야 하지 않을까도 생각했다.\u003c/li\u003e\n\u003cli\u003e5월의 절반이 지나가는 시점에서도 아직 업무가 구체화되지 않았다. 내가 하고 싶은 일이 정확히 뭔지 점점 더 아리쏭해진다.\u003c/li\u003e\n\u003cli\u003eCilium을 통한 노드간 iBGP 통신에 성공하고 싶다. 다만 오늘 그쪽에 신경 쓸 여력이 없었다. Cilium Up\u0026amp;Running 이라는 책이 만들어지고 있다는 정보를 습득했다.\u003c/li\u003e\n\u003cli\u003e공룡책(운영체제) 책도 진행하지 못했다. 어쩌다보니 드문드문 손이 가게된다. 습관적으로 조금씩 읽어나가야 할텐데 말이다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"이외의-생각\"\u003e이외의 생각\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e더워진다. 반팔옷을 입어야겠다.\u003c/li\u003e\n\u003cli\u003e마음을 다잡아야 겠다. 요즘 너무 심란한 상태에 있었다. 보는 방향을 바꿔야겠다.\u003c/li\u003e\n\u003cli\u003e운동을 해야한다. 퇴근길은 되도록이면 걸어가자. 다만, 걸으면서 들을만한 좋은 팟캐스트가 있으면 좋겠다.\u003c/li\u003e\n\u003cli\u003e글쓰기에는 꾸준함이 필요하다.\u003c/li\u003e\n\u003cli\u003e미니서버 3대를 구성하고 싶다. 오픈스택 용 컨트롤러 + 컴퓨트 + 스토리지 구성으로 \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e","tags":null,"title":"2025-05-14"},{"categories":null,"contents":"https://github.com/kubernetes/kubernetes\n쿠버네티스 딥 다이브를 시도하는 이유 시중에 쿠버네티스(이하 K8s)를 엔지니어 관점에서 활용하는 예시는 무궁무진하다. 배포, 모니터링, 유지 관리 등등 . 특히 K8s 클러스터를 배포하는 것은 kubeadm, kubespray, k3s, k0s, kind, minikube 등등 엄청나게 방법이 다양하다.\n하지만 K8s를 개발의 관점에서 접근하는 경우는 드물다. 당장 k8s 개발 환경 구축 키워드로 검색해보아도, 대부분 엔지니어 관점에서의 게시글이다.\n잘 만들어졌거나, 충분히 완성도가 높을 것이라 기대되는 K8s를 굳이 빌드해보는 이유는 무엇일까? 이에 대한 나의 선제적 답은 엔지니어와 개발자 두 가지 관점으로 있다.\n엔지니어의 관점에서, 소프트웨어 내부 구조와 동작 원리를 얼마나 깊이 이해하고 있느냐에 따라 그 도구를 사용하는 효율성은 크게 달라진다. 특히, 복잡한 시스템일수록 내부 로직을 깊이 이해하는 것은 단순한 활용 이상의 가치를 제공한다.\n이러한 이해는 단순히 현재의 요구 사항을 충족시키는 데서 그치지 않고, 예상치 못한 장애 상황이나 극한의 환경에서도 시스템의 성능과 안정성을 유지할 수 있는 확장성과 대응 능력을 제공한다. 예를 들어, 대규모 트래픽 폭증, 비정상적인 노드 장애, 또는 네트워크 단절과 같은 상황은 일반적인 환경에서는 쉽게 재현할 수 없지만, 이러한 시나리오에 대비한 설계와 배포 전략을 세우는 데에는 내부 로직에 대한 깊은 이해가 필수적이다.\n또한, K8s를 코드 수준으로 분석하면서 얻게 되는 경험은 단순히 작동 방식을 넘어, 시스템의 다양한 구성 요소가 어떻게 상호작용하는지, 이를 최적화하거나 커스터마이징하는 방법은 무엇인지에 대한 통찰력을 제공한다. 이는 특정 프로젝트의 요구 사항에 맞는 맞춤형 솔루션을 설계하거나, 시스템 병목을 제거하고 성능을 극대화하는 데 직접적인 도움을 준다.\n개발자 관점에서는 Kubernetes가 단순히 컨테이너 오케스트레이션 도구로서의 가치를 넘어 오픈소스 생태계와 Go 언어 기반 프로젝트의 대표적인 성공 사례로 꼽힌다는 점이다. 이는 단순히 소프트웨어를 빌드하고 사용하는 것을 넘어, 오픈소스 협업의 구조, 코드 품질 관리, 대규모 커뮤니티 기여 방식 등을 깊이 이해하는 데 훌륭한 케이스 스터디가 될 수 있다.\n특히, Kubernetes는 복잡하고 정교하게 설계된 코드베이스를 가지고 있어, Go 언어의 고급 활용 사례를 학습하고, 효율적이고 확장 가능한 소프트웨어 설계를 탐구하기에 적합하다. 이를 통해 단순히 Kubernetes 자체의 동작 원리를 배우는 것뿐만 아니라, 대규모 오픈소스 프로젝트의 개발 및 운영 방식을 체득할 수 있다.\n내 실력으로 쿠버네티스 생태계를 얼마나 깊은 수준으로 탐험할 수 있을까 걱정이지만, 조심스러운 여정을 시작해보려 한다.\n개발 환경 구축 구체적인 방법은 다음 주소에서 확인할 수 있다. https://github.com/kubernetes/community/blob/master/contributors/devel/README.md\nBuilding Kubernetes with Docker hack/local-up-cluster.sh 예시 : https://medium.com/@ElieXU/k8s-cluster-setup-from-source-code-9e38354a24fd 환경 셋업 아래 환경에서 프로젝트를 생성하여 진행하였다. cpu : 16 코어 mem : 60 os : rocky 9.4 go 1.23.4\n빌드 깃허브 레포지토리 readme를 확인하면 아래와 같은 두 가지 방법이 존재한다.\ngolang 기반 환경 git clone https://github.com/kubernetes/kubernetes cd kubernetes make 실제로 빌드 시에 시간 많이 소요되진 않는다. 거의 금방 완료된다. golang 기반으로 빌드 하였을 때 결과물은 다음과 같다.\n$ pwd /home/syyang/projects/kubernetes/_output/bin $ ls apiextensions-apiserver e2e.test go-runner kube-aggregator kube-controller-manager kubectl-convert kube-log-runner kube-proxy mounter e2e_node.test ginkgo kubeadm kube-apiserver kubectl kubelet kubemark kube-scheduler docker 환경 hack/local-up-cluster.sh 문서를 따라하기 위해 다음 방법을 사용한다.\ngit clone https://github.com/kubernetes/kubernetes cd kubernetes make quick-release 해당 방식은 10~15분 정도 시간이 소요 된다.\n$ make quick-release +++ [0120 14:01:32] Verifying Prerequisites.... +++ [0120 14:01:32] Building Docker image kube-build:build-9bf87f211d-5-v1.33.0-go1.23.4-bullseye.0 +++ [0120 14:05:22] Creating data container kube-build-data-9bf87f211d-5-v1.33.0-go1.23.4-bullseye.0 +++ [0120 14:05:22] Syncing sources to container +++ [0120 14:05:28] Running build command... +++ [0120 14:05:31] Building go targets for linux/amd64 k8s.io/apiextensions-apiserver (static) k8s.io/component-base/logs/kube-log-runner (static) k8s.io/kube-aggregator (static) k8s.io/kubernetes/cluster/gce/gci/mounter (static) k8s.io/kubernetes/cmd/kube-apiserver (static) k8s.io/kubernetes/cmd/kube-controller-manager (static) k8s.io/kubernetes/cmd/kube-proxy (static) k8s.io/kubernetes/cmd/kube-scheduler (static) k8s.io/kubernetes/cmd/kubeadm (static) k8s.io/kubernetes/cmd/kubelet (non-static) +++ [0120 14:06:39] Building go targets for linux/amd64 k8s.io/component-base/logs/kube-log-runner (static) k8s.io/kubernetes/cmd/kube-proxy (static) k8s.io/kubernetes/cmd/kubeadm (static) k8s.io/kubernetes/cmd/kubelet (non-static) +++ [0120 14:06:45] Building go targets for linux/amd64 k8s.io/kubernetes/cmd/kubectl (static) k8s.io/kubernetes/cmd/kubectl-convert (static) +++ [0120 14:06:51] Building go targets for linux/amd64 github.com/onsi/ginkgo/v2/ginkgo (non-static) k8s.io/kubernetes/test/conformance/image/go-runner (non-static) k8s.io/kubernetes/test/e2e/e2e.test (test) +++ [0120 14:07:13] Building go targets for linux/amd64 github.com/onsi/ginkgo/v2/ginkgo (non-static) k8s.io/kubernetes/cmd/kubemark (static) k8s.io/kubernetes/test/e2e_node/e2e_node.test (test) +++ [0120 14:07:34] Syncing out of container +++ [0120 14:07:37] Building tarball: src +++ [0120 14:07:37] Building tarball: manifests +++ [0120 14:07:37] Starting tarball: client linux-amd64 +++ [0120 14:07:37] Waiting on tarballs gtar: Removing leading `/\u0026#39; from member names gtar: Removing leading `/\u0026#39; from hard link targets gtar: Removing leading `/\u0026#39; from member names gtar: Removing leading `/\u0026#39; from member names gtar: Removing leading `/\u0026#39; from hard link targets gtar: Removing leading `/\u0026#39; from member names +++ [0120 14:07:42] Building tarball: node linux-amd64 +++ [0120 14:07:42] Building images: linux-amd64 +++ [0120 14:07:42] Starting docker build for image: kube-apiserver-amd64 +++ [0120 14:07:42] Starting docker build for image: kube-controller-manager-amd64 +++ [0120 14:07:42] Starting docker build for image: kube-scheduler-amd64 +++ [0120 14:07:42] Starting docker build for image: kube-proxy-amd64 +++ [0120 14:07:42] Starting docker build for image: kubectl-amd64 +++ [0120 14:07:46] Deleting docker image registry.k8s.io/kube-proxy-amd64:v1.33.0-alpha.0.536_8b1fe81dfa5ca3 +++ [0120 14:07:47] Deleting docker image registry.k8s.io/kubectl-amd64:v1.33.0-alpha.0.536_8b1fe81dfa5ca3 +++ [0120 14:07:47] Deleting docker image registry.k8s.io/kube-scheduler-amd64:v1.33.0-alpha.0.536_8b1fe81dfa5ca3 +++ [0120 14:07:47] Deleting docker image registry.k8s.io/kube-controller-manager-amd64:v1.33.0-alpha.0.536_8b1fe81dfa5ca3 +++ [0120 14:07:49] Deleting docker image registry.k8s.io/kube-apiserver-amd64:v1.33.0-alpha.0.536_8b1fe81dfa5ca3 +++ [0120 14:07:49] Docker builds done +++ [0120 14:07:49] Building tarball: server linux-amd64 +++ [0120 14:08:21] Building tarball: final +++ [0120 14:08:21] Waiting on test tarballs +++ [0120 14:08:21] Starting tarball: test linux-amd64 +++ [0120 14:08:30] Building tarball: test portable hack/local-up-cluster.sh 로컬환경에 클러스터를 배포하는 스크립트이다. etcd 가 설치되어 있어야 한다. 아래는 etcd가 없어서 새로 설치 후 다시 진행했다.\n$ export. CONTAINER_RUNTIME_ENDPOINT=\u0026#34;unix:///run/containerd/containerd.sock\u0026#34; $ sudo -E PATH=$PATH hack/local-up-cluster.sh make: Entering directory \u0026#39;/home/syyang/projects/kubernetes\u0026#39; +++ [0120 14:13:07] Building go targets for linux/amd64 k8s.io/kubernetes/cmd/cloud-controller-manager (non-static) k8s.io/kubernetes/cmd/kube-apiserver (static) k8s.io/kubernetes/cmd/kube-controller-manager (static) k8s.io/kubernetes/cmd/kubectl (static) k8s.io/kubernetes/cmd/kubelet (non-static) k8s.io/kubernetes/cmd/kube-proxy (static) k8s.io/kubernetes/cmd/kube-scheduler (static) make: Leaving directory \u0026#39;/home/syyang/projects/kubernetes\u0026#39; [sudo] password for syyang: etcd must be in your PATH You can use \u0026#39;hack/install-etcd.sh\u0026#39; to install a copy in third_party/. $ ./hack/install-etcd.sh Downloading https://github.com/etcd-io/etcd/releases/download/v3.5.17/etcd-v3.5.17-linux-amd64.tar.gz succeed PATH=\u0026#34;$PATH:/home/syyang/projects/kubernetes/third_party/etcd\u0026#34; $ PATH=\u0026#34;$PATH:/home/syyang/projects/kubernetes/third_party/etcd\u0026#34; /etc/containerd/config.toml 파일에서 SystemdCgroup = true 라인을 지워야 하고 ( 해당 환경에 과거 k8s 설치이력이 있었다면, 클러스터 성분 및 kubelet 등을 깔끔하게 제거 하는 것을 추천한다 ) sudo 권한을 사용하여 sudo -E PATH=$PATH hack/local-up-cluster.sh 로 실행하는 게 좋다.\n... pod/coredns-76b7578cff-2qlv5 condition met deployment.apps/coredns condition met 6 Create default storage class for storageclass.storage.k8s.io/standard created Local Kubernetes cluster is running. Press Ctrl-C to shut it down. Configurations: /tmp/local-up-cluster.sh.7bEbu8/kube-audit-policy-file /tmp/local-up-cluster.sh.7bEbu8/kube_egress_selector_configuration.yaml /tmp/local-up-cluster.sh.7bEbu8/kubelet.yaml /tmp/local-up-cluster.sh.7bEbu8/kube-proxy.yaml /tmp/local-up-cluster.sh.7bEbu8/kube-scheduler.yaml /tmp/local-up-cluster.sh.7bEbu8/kube-serviceaccount.key Logs: /tmp/etcd.log /tmp/kube-apiserver.log /tmp/kube-controller-manager.log /tmp/kube-proxy.log /tmp/kube-scheduler.log /tmp/kubelet.log To start using your cluster, you can open up another terminal/tab and run: export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig cluster/kubectl.sh Alternatively, you can write to the default kubeconfig: export KUBERNETES_PROVIDER=local cluster/kubectl.sh config set-cluster local --server=https://localhost:6443 --certificate-authority=/var/run/kubernetes/server-ca.crt cluster/kubectl.sh config set-credentials myself --client-key=/var/run/kubernetes/client-admin.key --client-certificate=/var/run/kubernetes/client-admin.crt cluster/kubectl.sh config set-context local --cluster=local --user=myself cluster/kubectl.sh config use-context local cluster/kubectl.sh 코드 수정 후 매번 이런 식으로 클러스터를 배포해 볼 수 있다. 다만 이 과정 자체가 금방 수행된다는 느낌은 아니고 일부 수정 후에 항상 전체를 재배포해야 한다는 불만 요소가 있다. kind 등을 통해 컨테이너 수준으로 배포시에 파일 일부 변화만 감지하여 재배포하는 프로세스 등을 시도해봄직 하다.\n우선은 간단한 방법론을 알아보았는데, 더 편리한 배포 방식은 추후에 더 심도있게 응용해야할 때가 있을 더 고민을 해볼 필요가 있을 듯하다.\nkind K8s in Docker 라는 이름의 뜻 https://kind.sigs.k8s.io kind supports building Kubernetes release builds from source 라는 문장을 문서 어딘가에서 확인하였는데, 실제로 변경된 코드 사항을 빠르게 클러스터로 곧장 배포할 수 있다면 쿠버네티스 개발에 큰 도움이 되지 않을까 싶다.\n다만, 아직 정확한 방법을 찾지는 못 하였는데, 예상컨데 소스 빌드 후 kind 노드 이미지를 새로 빌드하여 kind 클러스터를 배포하는 방식이 있을 듯하고, 가장 좋은 것은 배포된 환경에서 일부 컴포넌트만 수정된 컴포넌트로 교체하는 방식이 존재하는 것이다.\n공식 페이지 resource 카테고리 확인시, CI로 사용하는 예제에 대한 영상이 많다.\n코드 훑어보기 가장 먼저 K8s를 코드 수준으로 분석하기 위해 git 코드베이스를 펼쳐보았을 때 겪은 문제가 있었다. 코드를 어디서부터 확인해야 하는지 모르겠다는 점이다. K8s 레포의 구조는 다음과 같다.\n$ ls api CHANGELOG cluster code-of-conduct.md docs go.sum go.work.sum LICENSE logo _output OWNERS_ALIASES plugin SECURITY_CONTACTS SUPPORT.md third_party build CHANGELOG.md cmd CONTRIBUTING.md go.mod go.work hack LICENSES Makefile OWNERS pkg README.md staging test vendor golang의 표준 레포 레이아웃은 해당 주소를 확인해보면 좋다. 공식적인 표준은 아니라는 점을 주의해야 한다.\n위 게시글을 참고로 보았을 때, 코드를 확인할 때 가장 먼저 확인하면 좋은 영역은 역시 cmd 하위이다.\ncmd $ ls clicheck dependencyverifier genkubedocs genutils import-boss kube-apiserver kubectl-convert kube-proxy preferredimports cloud-controller-manager fieldnamedocscheck genman genyaml importverifier kube-controller-manager kubelet kube-scheduler prune-junit-xml dependencycheck gendocs genswaggertypedocs gotemplate kubeadm kubectl kubemark OWNERS yamlfmt 익숙한 컴포넌트와 익숙하지 않은 컴포넌트들이 동시에 눈에 띈다. 예를 들면, kube 로 시작하는 컴포넌트들은 익숙하다. kubeadm, kube-apiserver, kube-controller-manager, kubectl, kubelet, kube-proxy, kube-scheduler 등등이다.\n쿠버네티스를 구성하는 핵심 컴포넌트들 모두가 하나의 레포에 모여 있다. 이를 모노레포 전략이라고 부른다. OpenStack은 기본적으로 여러 서비스에 대한 관리를 멀티 레포지토리로 처리한다. 예를 들면, Nova 코드의 원리를 확인하고 싶으면 Nova 레포지토리를 확인하면 된다.\n눈에 익숙치 않은 컴포넌트 들도 존재한다. 이름으로 짐작컨데, 문서 작성이나 의존성 체크 등 프로젝트 관리에 도움이 되는 요소라고 추측한다.\n이 수 많은 컴포넌트 중에, 쿠버네티스 구조에서 가장 중요하다고 생각이 되는 kube-apiserver 안을 살펴보자. 생각보다 아기자기한 구성이다. apiserver.go 와 app 폴더가 메인이고 특징적으로 .import-restrictions 파일이 존재한다.\nls -al . .. apiserver.go app .import-restrictions OWNERS apiserver.go 파일은 main 패키지의 역할이다. 일반적인 오픈소스들의 코드 시작부는 정말 간단한 것 같다. 추후 프로젝트를 시작할 때 고려해 볼만한 사항이다.\n/* Copyright 2014 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ // APIServer is the main API server and master for the cluster. // It is responsible for serving the cluster management API. package main import ( \u0026#34;os\u0026#34; _ \u0026#34;time/tzdata\u0026#34; // for timeZone support in CronJob \u0026#34;k8s.io/component-base/cli\u0026#34; _ \u0026#34;k8s.io/component-base/logs/json/register\u0026#34; // for JSON log format registration _ \u0026#34;k8s.io/component-base/metrics/prometheus/clientgo\u0026#34; // load all the prometheus client-go plugins _ \u0026#34;k8s.io/component-base/metrics/prometheus/version\u0026#34; // for version metric registration \u0026#34;k8s.io/kubernetes/cmd/kube-apiserver/app\u0026#34; ) func main() { command := app.NewAPIServerCommand() code := cli.Run(command) os.Exit(code) } 핵심 로직은 \u0026quot;k8s.io/kubernetes/cmd/kube-apiserver/app\u0026quot; 쪽에 담겨 있음을 짐작할 수 있고, 프로그램의 시작이 cobra 기반의 cli tool로 되어 있다는 사실이 매력적이다. cobra 기반임을 알 수 있는 건 \u0026quot;k8s.io/component-base/cli\u0026quot; 쪽을 미리 확인했기 때문이다. 이를 별도의 component-base 경로의 cli 패키지로 정의했다는 것은 여러 프로젝트에서 공통적으로 사용하는 디자인이라고 생각할 수 있다.\npackage cli import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; cliflag \u0026#34;k8s.io/component-base/cli/flag\u0026#34; \u0026#34;k8s.io/component-base/logs\u0026#34; \u0026#34;k8s.io/klog/v2\u0026#34; ) staging 자연스럽게 staging 폴더로 넘어가보자. \u0026quot;k8s.io/component-base/cli\u0026quot; 파일 경로는 staging/src 폴더 하위에 있었다. staging 폴더는 Kubernetes 코드베이스의 모듈화 및 재사용성을 위한 중요한 구조적 요소이다.\nstaging 폴더에 있는 코드는 Kubernetes 코드베이스 내에서 개발 및 테스트되지만, 최종적으로는 외부 리포지토리(예: https://github.com/kubernetes/client-go)로 퍼블리싱 봇에 의해 복사된다.\nKubernetes 프로젝트는 staging 디렉토리에 정의된 모듈을 내부적으로 사용하며, Go 모듈의 replace 지시문을 활용하여 경로를 지정한다. 이를 통해 Kubernetes 프로젝트는 자체 코드를 의존성으로 처리하고, 외부에 독립적인 라이브러리로 제공할 수 있다.\nstaging 구조는 Kubernetes의 모노레포를 점진적으로 분리하여, 독립적인 마이크로 리포지토리로 전환하는 전략의 일부다. 이는 코드 재사용성과 커뮤니티 기여를 촉진하고, 특정 라이브러리의 업데이트가 Kubernetes 전체 코드베이스에 영향을 미치지 않도록 설계되었다.\n관련한 전체적인 개발 프로세스는 다음과 같다.\n개발자가 Kubernetes 메인 리포지토리에 코드 변경(PR 생성). staging 코드 변경 사항 감지: Publishing Bot이 변경을 감지하고, 동기화 규칙에 따라 작업 실행. 외부 리포지토리로 코드 복사: rules.yaml에 지정된 대로 외부 리포지토리에 PR 생성. 의존성 업데이트: 관련 리포지토리 간 의존성을 업데이트하고, 테스트를 통해 검증. 꽤나 멋진 구조라고 생각한다.\npkg cmd/kube-apiserver/.import-restrictions 를 체크해보면, k8s.io/kubernetes/pkg 항목이 존재하는 것을 볼 수 있다.\nrules: - selectorRegexp: k8s[.]io/kubernetes allowedPrefixes: - k8s.io/kubernetes/cmd/kube-apiserver - k8s.io/kubernetes/pkg - k8s.io/kubernetes/plugin - k8s.io/kubernetes/test/utils - k8s.io/kubernetes/third_party pkg 폴더는 golang에서 흔히 사용되는 레포지토리 레이아웃 관점에서 내부 구현 세부사항과 공통 유틸리티를 담는 공간이지만, 근본적으로 외부 사용자가 사용할 수 있는 코드 세트이다. 즉, 이 폴더는 프로젝트 내부와 외부 모두에서 재사용 가능한 코드를 패키지 형태로 제공한다.\n실제 사용 예시로 cmd/kube-apiserver/app/server.go 파일의 임포트 항목을 살펴보면 다음과 같다. 거의 대부분의 임포트 항목의 경로에 pkg 폴더가 함유 됨을 알 수 있다.\npackage app import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/url\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; apiextensionsv1 \u0026#34;k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1\u0026#34; utilerrors \u0026#34;k8s.io/apimachinery/pkg/util/errors\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; \u0026#34;k8s.io/apiserver/pkg/admission\u0026#34; genericapifilters \u0026#34;k8s.io/apiserver/pkg/endpoints/filters\u0026#34; genericapiserver \u0026#34;k8s.io/apiserver/pkg/server\u0026#34; \u0026#34;k8s.io/apiserver/pkg/server/egressselector\u0026#34; serverstorage \u0026#34;k8s.io/apiserver/pkg/server/storage\u0026#34; utilfeature \u0026#34;k8s.io/apiserver/pkg/util/feature\u0026#34; \u0026#34;k8s.io/apiserver/pkg/util/notfoundhandler\u0026#34; \u0026#34;k8s.io/apiserver/pkg/util/webhook\u0026#34; clientgoinformers \u0026#34;k8s.io/client-go/informers\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; cliflag \u0026#34;k8s.io/component-base/cli/flag\u0026#34; \u0026#34;k8s.io/component-base/cli/globalflag\u0026#34; \u0026#34;k8s.io/component-base/featuregate\u0026#34; \u0026#34;k8s.io/component-base/logs\u0026#34; logsapi \u0026#34;k8s.io/component-base/logs/api/v1\u0026#34; _ \u0026#34;k8s.io/component-base/metrics/prometheus/workqueue\u0026#34; \u0026#34;k8s.io/component-base/term\u0026#34; utilversion \u0026#34;k8s.io/component-base/version\u0026#34; \u0026#34;k8s.io/component-base/version/verflag\u0026#34; \u0026#34;k8s.io/klog/v2\u0026#34; aggregatorapiserver \u0026#34;k8s.io/kube-aggregator/pkg/apiserver\u0026#34; \u0026#34;k8s.io/kubernetes/cmd/kube-apiserver/app/options\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/capabilities\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controlplane\u0026#34; controlplaneapiserver \u0026#34;k8s.io/kubernetes/pkg/controlplane/apiserver\u0026#34; \u0026#34;k8s.io/kubernetes/pkg/controlplane/reconcilers\u0026#34; kubeapiserveradmission \u0026#34;k8s.io/kubernetes/pkg/kubeapiserver/admission\u0026#34; ) golang을 다루다보면 패키지의 순환 의존성을 조심할 필요가 있다. 특정 패키지에서 호출(import)되는 패키지는, 자신을 호출하는 패키지를 호출 하면 안된다. 특히 모노레포 생태계인 K8s 생태계에서는 조심해야한 성질이며, 이러한 부분을 아마도 현명하게 컨트롤하고 있을 것이라 생각한다. .import-restrictions 가 그 노력 중 하나이다. 빌드 프로세스에서 위반 여부를 자동으로 검증 한다고 한다.\n또한, 패키지 간 호출의 변화는 코드 리뷰 프로세스를 통해 검증 된다. OWNERS 파일 에서는 패키지 간의 관리자를 정하고 있는데, 특정 디렉터리와 관련된 코드 변경은 해당 디렉터리의 전문가(Reviewer 또는 Approver)에 의해 검토되게 되는 시스템이다. 대표적으로 pkg/api/OWNERS 를 살펴보면 다음과 같다.\n# See the OWNERS docs at https://go.k8s.io/owners # Disable inheritance as this is an api owners file options: no_parent_owners: true filters: \u0026#34;.*\u0026#34;: approvers: - api-approvers reviewers: - api-reviewers # examples: # pkg/api/types.go # pkg/api/*/register.go \u0026#34;([^/]+/)?(register|types)\\\\.go$\u0026#34;: labels: - kind/api-change 패키지 구조는 가급적이면 2~3단계 이상의 폴더보다 더 깊게 생성되지 않는다. 또한 테스트 파일도 많이 작성해 둔다. kubernetes/pkg/api 경로를 예를 들면 이러하다.\n$ tree . . ├── endpoints │ └── testing │ └── make.go ├── job │ ├── warnings.go │ └── warnings_test.go ├── legacyscheme │ └── scheme.go ├── node │ ├── util.go │ └── util_test.go ├── OWNERS ├── persistentvolume │ ├── util.go │ └── util_test.go ├── persistentvolumeclaim │ ├── OWNERS │ ├── util.go │ └── util_test.go ├── pod │ ├── OWNERS │ ├── testing │ │ └── make.go │ ├── util.go │ ├── util_test.go │ ├── warnings.go │ └── warnings_test.go ├── service │ ├── OWNERS │ ├── testing │ │ └── make.go │ ├── util.go │ ├── util_test.go │ ├── warnings.go │ └── warnings_test.go ├── servicecidr │ ├── servicecidr.go │ └── servicecidr_test.go ├── storage │ ├── util.go │ └── util_test.go ├── testing │ ├── applyconfiguration_test.go │ ├── backward_compatibility_test.go │ ├── compat │ │ └── compatibility_tester.go │ ├── conversion.go │ ├── conversion_test.go │ ├── copy_test.go │ ├── deep_copy_test.go │ ├── defaulting_test.go │ ├── doc.go │ ├── fuzzer.go │ ├── install.go │ ├── meta_test.go │ ├── node_example.json │ ├── OWNERS │ ├── replication_controller_example.json │ ├── serialization_proto_test.go │ ├── serialization_test.go │ └── unstructured_test.go └── v1 ├── endpoints │ ├── util.go │ └── util_test.go ├── OWNERS ├── persistentvolume │ ├── util.go │ └── util_test.go ├── pod │ ├── util.go │ └── util_test.go ├── resource │ ├── helpers.go │ └── helpers_test.go └── service ├── util.go └── util_test.go 이외 위에서 살펴본 폴더 이외에도 test, hack, build, api, plugin 등등의 여러 폴더가 존재한다. 재미 삼아 api/openapi-spec/swagger.json 파일을 열어보면, 8만 줄에 가까운 엄청난 길의 파일도 만나볼 수 있다. ( 아마도 자동으로 생성될 것이라 예상한다 )\nK8s는 탄생 10주년을 맞이하는 정말 방대한 프로젝트이다. 한 번에 모든 컴포넌트를 분석할 순 없다. 처음부터 코드를 분석하는 것은 아주 어려운 일이다. 거의 대부분의 경우 필요에 의해 코드를 찾아가는 것이 일반적이다. 그러다 우연히 처음 보는 영역에 발을 들이는 순간에 각 폴더 별로 존재하는 README.md 파일을 첫 지침서로서 활용하는 편이 좋겠다.\n정리 Kubernetes는 컨테이너 오케스트레이션을 넘어 분산 시스템의 다양한 요구를 충족시키는 플랫폼이다. 이번 글에서는 Kubernetes 코드베이스의 주요 구조를 살펴보았으며, cmd, staging, pkg 디렉터리의 역할과 목적을 분석했다. 이를 통해 Kubernetes 내부 구조를 이해하고, 필요에 따라 기능을 커스터마이징하거나 디버깅하는 데 활용할 수 있다.\nKubernetes를 제대로 이해하려면 코드 구조를 아는 것에서 그치지 않고, 클러스터가 실제로 동작하는 원리를 탐구해야 한다. 특히, “파드 생성 과정”은 Kubernetes의 핵심 기능 중 하나로, 클러스터의 주요 컴포넌트가 어떻게 상호작용하며 워크로드를 배포하는지를 보여주는 중요한 흐름이다.\n다음 글에서는 Kubernetes의 파드 생성 과정을 다루며, 클러스터 내부에서 일어나는 동작 원리를 구체적으로 파악 해보고자 한다.\n","date":"May 14, 2025","hero":"/images/hero/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EA%B0%80%20%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C.png","permalink":"https://kubesy.com/posts/300.-study/301.-kubernetes-deep-dive/hello-k8s-dev-world/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/kubernetes/kubernetes\" target=\"_blank\" rel=\"noopener\"\u003ehttps://github.com/kubernetes/kubernetes\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"쿠버네티스-딥-다이브를-시도하는-이유\"\u003e쿠버네티스 딥 다이브를 시도하는 이유\u003c/h3\u003e\n\u003cp\u003e시중에 쿠버네티스(이하 K8s)를 엔지니어 관점에서 활용하는 예시는 무궁무진하다. 배포, 모니터링, 유지 관리 등등 .\n특히 K8s 클러스터를 배포하는 것은 kubeadm, kubespray, k3s, k0s, kind, minikube 등등 엄청나게 방법이 다양하다.\u003c/p\u003e\n\u003cp\u003e하지만 K8s를 개발의 관점에서 접근하는 경우는 드물다. 당장 \u003ccode\u003ek8s 개발 환경 구축\u003c/code\u003e 키워드로 검색해보아도, 대부분 엔지니어 관점에서의 게시글이다.\u003c/p\u003e\n\u003cp\u003e잘 만들어졌거나, 충분히 완성도가 높을 것이라 기대되는 K8s를 굳이 빌드해보는 이유는 무엇일까? 이에 대한 나의 선제적 답은 엔지니어와 개발자 두 가지 관점으로 있다.\u003c/p\u003e","tags":null,"title":"1. Hello, k8s Dev World!"},{"categories":null,"contents":"들어가기 앞서 \u0026hellip; 저는 비전공자 출신이고, 이제 곧 만 3년의 경력을 채우는 클라우드 도메인 쪽 개발자 입니다. ( 사실 개발자와 DevOps 엔지니어 그 어딘가에 걸쳐 있다고 생각하긴 하는데 \u0026hellip; )\n현재 제 주력 언어는, go와 python 인데요. 예전부터 java spring을 그래도 한 번은 경험 해봐야겠다는 생각을 가지고 있었습니다. (한국에서 백엔드는 자바 밖에 안 뽑아요 \u0026hellip;)\n그러다보니 스프링 부트 핵심 가이드 라는 책을 이전에 사놓고 방치해두고 있었다가, 연말 휴가를 맞이하여 한 번 도전해봐야겠다는 생각이 들었습니다.\n책을 내용을 기반으로 정리하되, 저만의 색깔을 덧입혀서 글을 써보려고 합니다. 아무쪼록 저와 비슷한 상태에 있는 독자 분들에게 도움이 되길 바랍니다.\n개념 자바 스프링 부트는 자바 스프링을 좀 더 쉽게 다룰 수 있게 합니다. 자바 스프링 만으로 프로젝트를 구축하려면, 아주 복잡한 설정들을 수행해야 하는데, 스프링 부트를 사용하면 이를 보다 약소화 할 수 있습니다.\nSpring 이라는 이름의 의미는, 개발자들에게 봄이 왔다는 뜻이라나 \u0026hellip;\n스프링의 가장 큰 특징은 의존성 주입 입니다. 스프링의 의존성 주입은 객체 간 결합도를 줄이고, 유연성과 테스트 가능성을 높이는 핵심 개념입니다. 이를 통해 스프링 애플리케이션은 더 관리하기 쉽고 확장 가능한 구조를 가지게 됩니다. 스프링의 DI는 현대적인 애플리케이션 설계에서 매우 중요한 역할을 합니다.\nDI는 \u0026ldquo;제어의 역전\u0026rdquo;(Inversion of Control, IoC) 원칙을 기반으로 합니다. 객체의 생성을 개발자가 직접 제어하지 않고, 스프링 컨테이너가 관리하도록 함으로써 코드의 유연성과 확장성을 보장합니다.\n결합도 감소\n객체 간의 의존 관계를 코드에서 직접 명시하지 않아 유연성이 높아집니다. 한 객체의 변경이 다른 객체에 미치는 영향을 최소화합니다. 테스트 용이성\nDI를 통해 의존성을 주입받으므로, 테스트 시에는 **모의 객체(Mock)**를 쉽게 주입할 수 있습니다. 유닛 테스트와 통합 테스트를 보다 쉽게 수행할 수 있습니다. 재사용성 증가\n객체가 특정 구현체에 의존하지 않으므로, 인터페이스나 추상 클래스만 정의하면 다양한 구현체를 주입받아 사용할 수 있습니다. 객체 생명주기 관리\n스프링 컨테이너가 객체의 생성, 초기화, 소멸까지 관리하므로 코드가 간결해지고 유지보수가 쉬워집니다. IoC 컨테이너는 애플리케이션의 객체(빈)를 생성하고, 의존 관계를 설정한 뒤 애플리케이션이 실행될 때 이를 제공합니다.\n처리 과정:\n빈(Bean) 정의 @Component, @Service, @Repository 등 애노테이션을 사용해 스프링이 관리할 빈을 정의합니다. 의존 관계 설정 @Autowired 또는 생성자를 통해 의존성을 정의합니다. 빈 생성 스프링 컨테이너가 애플리케이션 시작 시점에 빈을 생성하고 의존성을 주입합니다. DI 의 구현 방식은 크게 세 가지로 나뉩니다. 다만 대부분의 경우, 생성자 주입이 권장 됩니다.\n생성자 주입 @Component public class Service { private Repository repository; @Autowired public void setRepository(Repository repository) { this.repository = repository; } } Setter 주입 @Component public class Service { @Autowired private Repository repository; } 필드 주입 @Component public class Service { @Autowired private Repository repository; } DI 이외에도 AOP(Aspect-Oriented Programming 와 같은 특징이 있다고 합니다.\n사실 여기까지만 읽었을 때, 구체적인 구현을 보기 전까지는 조금 모호하다라는 생각이 듭니다. 또 Go를 제 첫 언어로 삼아서 그런지 자꾸 비교를 해가면서 이해하게 되는 양상이 있습니다. 예를 들어 MVC 와 같은 개념은 golang에서도 은연 중 사용하고 있었을 지 모르지만, 그다지 본격적으로 개념부터 알고 들어가진 않았어었습니다.\n아래의 그림을 보면, 데이터베이스 강의 앞 부분에서 보았던 개념과 매칭됩니다.\nGo 에서의 웹개발(gin-gonic)과, Java Spring Boot 를 통한 웹개발을 비교해가며 학습할 때, 개념적 접근의 차이가 느껴집니다. 예를들어 Java Spring Boot 에서 논하는 MVC 등이 Go를 사용할 때는 엄밀하게 따져가면서 개발하지 않았기 때문입니다. 사실 어느 정도는 비슷한 컨셉을 공유하고 있을 것 같은데, 이러한 차이는 어디서 오는 걸까요? Java (Spring Boot):\nJava는 객체지향 언어로 설계되어 있으며, 복잡한 비즈니스 로직을 계층화된 방식으로 구성하는 데 강점을 가집니다. Spring Boot는 엔터프라이즈 환경에서 확립된 MVC(Model-View-Controller) 패턴을 기반으로 한 구조를 강조하며, 컨벤션을 통해 개발을 간소화하려는 목적을 갖고 있습니다. 프레임워크 자체에서 DI(Dependency Injection), AOP(Aspect-Oriented Programming), 그리고 풍부한 어노테이션 지원 등을 제공해 복잡한 구조를 깔끔하게 관리하도록 돕습니다. Go (gin-gonic):\nGo는 최소주의(minimalism)를 기반으로 설계된 언어입니다. 코드가 단순하고 읽기 쉬우며, 복잡한 추상화보다는 실용적인 접근을 강조합니다. gin-gonic은 Go 언어의 철학을 따르는 경량 웹 프레임워크로, 불필요한 구조를 강요하지 않고, 개발자가 직접 필요한 패턴(MVC 포함)을 구성하도록 허용합니다. Go에서는 보통 \u0026ldquo;프로젝트 구조를 단순하게 유지\u0026quot;하는 것이 기본 철학에 가깝기 때문에, 엄격한 MVC 구조가 필요 없을 수도 있습니다. 위는 디자인 패턴 종류를 분류 해놓은 것 입니다. 스프링은 이중에 싱글톤 패턴, 팩토리 메서드, 프록시, 템플릿 메서드, 어댑터 등등의 다양한 원칙을 내재화 있다고 합니다.\n환경 구축 반골기질 때문에, 모두가 인텔리제이를 주장할 때 본인은 vscode를 고집하고 싶습니다. 그리고 패키지들을 무조건 최신에 가깝게 설정하고 싶어요 참고자료 https://it-ability.tistory.com/92 https://huimang2.github.io/etc/ubuntu-dev-env vscode 확장 프로그램 다운로드 Spring Boot Extension Pack Java Extension Pack JDK 설치 $ sudo dnf install java-17-openjdk java-17-openjdk-devel $ java --version openjdk 17.0.13 2024-10-15 LTS OpenJDK Runtime Environment (Red_Hat-17.0.13.0.11-1) (build 17.0.13+11-LTS) OpenJDK 64-Bit Server VM (Red_Hat-17.0.13.0.11-1) (build 17.0.13+11-LTS, mixed mode, sharing) $ readlink -f /usr/bin/java /usr/lib/jvm/java-17-openjdk-17.0.13.0.11-4.el9.x86_64/bin/java $ vi ~/.bashrc ... export JAVA_HOME=/usr/lib/jvm/java-17-openjdk export PATH=$PATH:$JAVA_HOME/bin ... $ source ~/.bashrc https://start.spring.io 에서 프로젝트 설정\nGenerate 후 폴더 압축 해제. -\u0026gt; vscode 에서 java projects 라는 ui에서 생성해도 됩니다. 참고로 위의 Package 네임으로 설정하면 문법 오류인가 봅니다.\nvscode settings.json 에 아래 와 같이 설정해줘서 자바 home 경로를 알 수 있게 해주어야 합니다.\n{ \u0026#34;java.jdt.ls.java.home\u0026#34;: \u0026#34;/usr/lib/jvm/java-17-openjdk\u0026#34; } 자 이제 생성된 코드 환경에서 몇 가지 구성을 확인해봅시다.\n첫 번째로 gradle 관련 항목입니다. 그 다음은 Spring Boot Dashboard 입니다. 현재 환경은 다른 프로세스에서 8080 포트를 사용하고 있습니다. 기본 시작 포트를 18080으로 바꿔봅니다.\nsrc/main/resources/application.properties 파일을 수정합니다.\nspring.application.name=demo server.port=18080 구동! src/main/java/com/example/demo/controller/DemoController.java 파일을 만들어서 아래처럼 코드를 추가 해줍니다.\npackage com.example.demo.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class DemoController { @RequestMapping(\u0026#34;/demo\u0026#34;) public String hello() { return \u0026#34;Hello, World!\u0026#34;; } } 몇 가지 포인트를 찾아보면, 두 가지의 어노테이션을 임포트하였고, /demo라는 경로로 들어가는 경우 Hello, World! 를 리턴해주는 정말 간단한 로직입니다.\n솔직하게, import org.springframework.web.bind.annotation.RequestMapping; 등을 써야한다는 사실 자체 처음 알았고, 그 내부 로직이 무엇인지는 모릅니다. 그저 예제를 따라했을 뿐인데요.\n이러한 요소들을 파악하고, 좀 더 깊게 들어가면 기능의 구체적 구현 방식까지도 확인하는 그런 과정이 스프링을 이해하는 과정이 아닐까 싶은 생각이 들었습니다.\n","date":"December 24, 2024","hero":"/images/hero/spring%20boot.png","permalink":"https://kubesy.com/posts/300.-study/303.-java-spring-boot-/1.-%EA%B0%9C%EB%85%90-%EB%B0%8F-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%B6%95-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EA%B0%84%EB%8B%A8%ED%95%9C-%EA%B5%AC%EB%8F%99/","summary":"\u003ch3 id=\"들어가기-앞서-\"\u003e들어가기 앞서 \u0026hellip;\u003c/font\u003e\u003c/h3\u003e\n\u003cp\u003e저는 비전공자 출신이고, 이제 곧 만 3년의 경력을 채우는 클라우드 도메인 쪽 개발자 입니다. ( 사실 개발자와 DevOps 엔지니어 그 어딘가에 걸쳐 있다고 생각하긴 하는데 \u0026hellip; )\u003c/p\u003e\n\u003cp\u003e현재 제 주력 언어는, go와 python 인데요. 예전부터 java spring을 그래도 한 번은 경험 해봐야겠다는 생각을 가지고 있었습니다. (한국에서 백엔드는 자바 밖에 안 뽑아요 \u0026hellip;)\u003c/p\u003e\n\u003cp\u003e그러다보니 \u003ccode\u003e스프링 부트 핵심 가이드\u003c/code\u003e 라는 책을 이전에 사놓고 방치해두고 있었다가, 연말 휴가를 맞이하여 한 번 도전해봐야겠다는 생각이 들었습니다.\u003c/p\u003e","tags":null,"title":"1. 개념 및 환경 구축 그리고 간단한 구동"},{"categories":null,"contents":"개요 : https://operatorframework.io/\nRed Hat이 주도하는 오픈소스 프로젝트로, 오퍼레이터 개발을 단순화하고 자동화하기 위한 다양한 도구와 라이브러리를 제공합니다. Operator SDK는 Kubebuilder 기반으로 만들어졌지만, Helm, Ansible 등 다양한 언어와 프레임워크를 지원합니다. 또한 레드햇이 주도하여 개발 중으로 보이는 openstack operator 프로젝트에 사용되었습니다.\n쿠버네티스의 오퍼레이터와 오퍼레이터 패턴은 공식 문서 https://kubernetes.io/ko/docs/concepts/extend-kubernetes/operator/에 정리되어 있습니다.\n오퍼레이터(Operator)는 사용자 정의 리소스를 사용하여 애플리케이션 및 해당 컴포넌트를 관리하는 쿠버네티스의 소프트웨어 익스텐션이다. 오퍼레이터는 쿠버네티스 원칙, 특히 컨트롤 루프를 따른다.\n쿠버네티스 코드 자체를 수정하지 않고 기능을 확대한다는 점에서 큰 이점을 가지고 있습니다. 이미 cncf 의 많은 프로젝트들이 해당 기법을 사용하여 기능을 제공하고 있습니다. CKA 시험에서도 24년 11월 25일 이후부터 시험항목에 관련 내용이 추가됩니다.\nOperator Framework 는 크게 세 가지 특성 Operator SDK(build, test, iterate), Operator Lifecycle manager(install, manage, update), Operatorhub.io (Publish \u0026amp; share) 을 제공 합니다.\n이번 글에서는 코드 수준의 오퍼레이터 코드 작성 및 빌드 방식에 집중합니다.\nkubebuilder 구조 operator framework 가 내포한 kubebuilder는 아래와 같은 구조를 가지고 있습니다.\nkubebuilder 는 controller-runtime 이 핵심 라이브러리 입니다. [ https://github.com/kubernetes/sample-controller/blob/master/controller.go 이런 예시 프로젝트가 code-generator 기반 인 것과 비교할 수 있습니다. ]\n이미지에서 controller-runtime의 각 구성 요소(Client, Cache, Controller, Predicate, Reconciler, Webhook)에 대한 설명을 아래에 정리했습니다:\n1. Client 위치: sigs.k8s.io/controller-runtime/pkg/client\n역할:\nKubernetes API 서버와 통신하는 인터페이스로, API 요청을 전송합니다. 리소스를 조회(Create, Get, List), 수정(Update, Patch), 삭제(Delete)할 수 있는 메서드를 제공합니다. 캐시된 데이터를 조회할 수도 있으며, 필요한 경우 API 서버로 직접 요청을 보냅니다. 특징:\n캐시된 데이터를 먼저 조회하여 성능을 최적화합니다. 인증 및 프로토콜 처리를 자동으로 수행합니다. 2. Cache 위치: sigs.k8s.io/controller-runtime/pkg/cache\n역할:\n최근 Watch 또는 GET 요청에서 조회된 리소스를 저장하는 메모리 내 캐시입니다. Informer를 사용하여 Kubernetes 리소스를 캐싱하고, 변경 사항을 지속적으로 업데이트합니다. 특징:\nController와 Webhook에서 공통적으로 사용됩니다. 캐시를 통해 API 서버와의 통신 횟수를 줄이고, 성능을 높입니다. 3. Controller 위치: sigs.k8s.io/controller-runtime/pkg/controller\n역할:\n리소스의 변경 사항을 감지하고, 이를 Reconciler에 전달하여 리소스 상태를 관리합니다. 각 CRD(Custom Resource Definition)마다 하나의 Controller가 생성됩니다. 특징:\n이벤트 필터링(Filtering)을 통해 필요한 이벤트만 처리할 수 있습니다. 큐(Queue)를 사용하여 백오프(Backoff) 및 재시도(Re-queuing)를 관리합니다. Reconciler를 호출하여 상태를 조정하는 로직을 실행합니다. 4. Predicate 위치: sigs.k8s.io/controller-runtime/pkg/predicate\n역할:\n이벤트를 필터링하여 Reconciler에 전달할 필요가 있는지 여부를 결정합니다. 특징:\nCreate, Update, Delete, Generic 이벤트에 대해 필터링 로직을 구현할 수 있습니다. 예를 들어, 특정 레이블이 있는 리소스의 변경 사항만 처리하거나, 특정 필드 값의 변경만 감지할 수 있습니다. 5. Reconciler 위치: sigs.k8s.io/controller-runtime/pkg/reconcile\n역할:\n리소스의 현재 상태와 원하는 상태를 비교하고, 이를 동기화하는 로직을 구현합니다. 사용자 정의 Reconciler는 Reconcile 메서드를 구현해야 합니다. 특징:\nReconcile 메서드는 이벤트가 발생할 때마다 호출되며, 리소스를 업데이트하거나 다른 API 요청을 전송할 수 있습니다. 주로 idempotent(멱등성)을 유지하도록 설계되어야 합니다. Reconcile 메서드의 반환 값은 reconcile.Result로, 재시도 여부와 간격을 결정할 수 있습니다. 6. Webhook 위치: sigs.k8s.io/controller-runtime/pkg/webhook\n역할:\n리소스의 Admission Request(생성, 업데이트 요청)를 가로채어 검증 및 수정 작업을 수행합니다.\n종류:\nDefaulter: 리소스의 기본값을 설정합니다. 필드 값이 비어 있을 경우 기본값을 채워 넣습니다. Validator: 리소스의 유효성을 검사합니다. 잘못된 형식이나 비정상적인 값이 있는 경우 요청을 거부합니다. 특징:\nKubernetes API 서버와의 통신을 통해 AdmissionRequest를 처리합니다. 사용자 정의 Webhook을 통해 리소스 검증 및 수정 로직을 추가할 수 있습니다. 문서 따라하기 https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/ 문서를 따라 합니다.\noverview 다음 프로젝트를 수행합니다. Memcached 디플로이먼트가 존재하지 않는다면, 생성합니다. 커스텀 리소스 스펙에 정의된 isk 크기와 동일한 상황에 있는 것을 보장합니다. Memcached CR의 상태를 Status Writer를 통해 해당 CR의 파드 이름을 사용하여 업데이트 합니다. 설치 **Compile and install from master** ```bash git clone https://github.com/operator-framework/operator-sdk cd operator-sdk git checkout master make install ``` 프로젝트 생성 mkdir memcached-operator cd memcached-operator operator-sdk init --domain example.com --repo github.com/example/memcached-operator 환경 확인\n~/memcached-operator $ ls cmd config Dockerfile go.mod go.sum hack Makefile PROJECT README.md test --domain 은 API group의 prefix가 됩니다. k8s에서의 대표적 그룹 예시는 apps 나 rbac.authoriztion.k8s.io 가 있습니다. 좀 더 구체적인 내용(쿠버네티스의 api를 이해하는 데 도움이 됩니다)은 https://book.kubebuilder.io/cronjob-tutorial/gvks.html 에서 확인할 수 있습니다. 또한 생성된 프로젝트의 기본 구조는 https://book.kubebuilder.io/cronjob-tutorial/basic-project.html 문서를 확인하면 좋습니다.\ngo.mod Makefile : 프로젝트 작업에 필요한 여러 명령어들이 정리되어 있습니다. 해당 파일을 분석하면 어떤 작업을 할 수 있는지 대략적으로 감을 잡을 수 있습니다. PROJECT : Kubebuilder 가 사용할 메타데이터 입니다. 새로운 구성요소를 scaffolding (리소스를 빠르게 초기화하고 설정할 수 있는 템플릿이나 자동화된 생성 도구를 의미) 하는 데 필요 합니다. config 폴더 아래에는 Kustomize YAML 정의들을 가지고 있고, 추후에 컨트롤러를 작성하기 시작하면, 커스텀 리소스 정의나 RBAC 설정 그리고 웹훅 설정 등도 생성되게 됩니다.\n참고로 --repo=\u0026lt;path\u0026gt; 설정은 $GOPATH/src 밖에서 프로젝트를 생성할 때 사용 되어야 합니다.\nManager cmd/main.go 는 오퍼레이터의 메인 프로그램이고 Manager를 초기화하고 구동 시킵니다.\n매니저는 네임스페이스의 제한 둘 수 있습니다. 해당 네임스페이스에서만 모든 컨트롤러는 리소스를 watch 할 수 있습니다.\nmgr, err := ctrl.NewManager(cfg, manager.Options{Namespace: namespace}) 만약 모든 네임스페이스를 가능하게 하려면, 빈 값으로 두면 됩니다.\nmgr, err := ctrl.NewManager(cfg, manager.Options{Namespace: \u0026#34;\u0026#34;}) Operator scope에 대한 더 심도있는 내용은 https://sdk.operatorframework.io/docs/building-operators/golang/operator-scope/ 문서를 살펴보아야 합니다.\nCreate a new API and Controller\n~/memcached-operator $ operator-sdk create api --group cache --version v1alpha1 --kind Memcached --resource --controller INFO[0000] Writing kustomize manifests for you to edit... INFO[0000] Writing scaffold for you to edit... INFO[0000] api/v1alpha1/memcached_types.go INFO[0000] api/v1alpha1/groupversion_info.go INFO[0000] internal/controller/suite_test.go INFO[0000] internal/controller/memcached_controller.go INFO[0000] internal/controller/memcached_controller_test.go INFO[0000] Update dependencies: $ go mod tidy INFO[0000] Running make: $ make generate mkdir -p /home/syyang/memcached-operator/bin Downloading sigs.k8s.io/controller-tools/cmd/controller-gen@v0.15.0 go: downloading sigs.k8s.io/controller-tools v0.15.0 go: downloading k8s.io/api v0.30.0 go: downloading k8s.io/apiextensions-apiserver v0.30.0 go: downloading k8s.io/apimachinery v0.30.0 go: downloading golang.org/x/tools v0.20.0 go: downloading golang.org/x/sys v0.19.0 go: downloading golang.org/x/net v0.24.0 /home/syyang/memcached-operator/bin/controller-gen object:headerFile=\u0026#34;hack/boilerplate.go.txt\u0026#34; paths=\u0026#34;./...\u0026#34; Next: implement your new API and generate the manifests (e.g. CRDs,CRs) with: $ make manifests 환경 확인\n~/memcached-operator $ ls api cmd Dockerfile go.sum internal PROJECT test bin config go.mod hack Makefile README.md api/v1alpha1/memcached_types.go 를 통해 api 리소스가 만들어지고 controllers/memcached_controller.go 를 통해 컨트롤러가 만들어집니다. (정확히는 scaffold 합니다)\n참고\n더 완벽한 튜토리얼을 위해서는 아래 명령을 시도해볼 수 있습니다.\n$ operator-sdk create api --group cache --version v1alpha1 --kind Memcached --plugins=\u0026#34;deploy-image/v1-alpha\u0026#34; --image=memcached:1.4.36-alpine --image-container-command=\u0026#34;memcached,-m=64,modern,-v\u0026#34; --run-as-user=\u0026#34;1001\u0026#34; 이번 예제는 single group API 경우에 대해서 다룹니다. multi-group 은 https://book.kubebuilder.io/migration/multi-group.html 문서를 확인해야 합니다.\ncontroller-runtime에서 설정한 설계 목표를 제대로 따르기 위해, 프로젝트에서 생성된 각 API를 관리하는 전용 컨트롤러를 하나씩 두는 것이 권장됩니다.\nDefine the API\n먼저, Memcached 타입을 정의하여 API를 표현할 것입니다. 이 타입은 MemcachedSpec.Size 필드를 통해 배포할 Memcached 인스턴스(CR)의 수를 설정하고, MemcachedStatus.Conditions 필드를 통해 CR의 상태(Conditions)를 저장할 것입니다.\napi/v1alpha1/memcached_types.go 파일에서 Go 타입 정의를 수정하여 Memcached Custom Resource(CR)의 API에 다음과 같은 spec과 status를 추가하면 됩니다.\n// MemcachedSpec defines the desired state of Memcached type MemcachedSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // The following markers will use OpenAPI v3 schema to validate the value // More info: https://book.kubebuilder.io/reference/markers/crd-validation.html // +kubebuilder:validation:Minimum=1 // +kubebuilder:validation:Maximum=5 // +kubebuilder:validation:ExclusiveMaximum=false // Size defines the number of Memcached instances // +operator-sdk:csv:customresourcedefinitions:type=spec Size int32 `json:\u0026#34;size,omitempty\u0026#34;` // Port defines the port that will be used to init the container with the image // +operator-sdk:csv:customresourcedefinitions:type=spec ContainerPort int32 `json:\u0026#34;containerPort,omitempty\u0026#34;` } // MemcachedStatus defines the observed state of Memcached type MemcachedStatus struct { // Represents the observations of a Memcached\u0026#39;s current state. // Memcached.status.conditions.type are: \u0026#34;Available\u0026#34;, \u0026#34;Progressing\u0026#34;, and \u0026#34;Degraded\u0026#34; // Memcached.status.conditions.status are one of True, False, Unknown. // Memcached.status.conditions.reason the value should be a CamelCase string and producers of specific // condition types may define expected values and meanings for this field, and whether the values // are considered a guaranteed API. // Memcached.status.conditions.Message is a human readable message indicating details about the transition. // For further information see: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties // Conditions store the status conditions of the Memcached instances // +operator-sdk:csv:customresourcedefinitions:type=status Conditions []metav1.Condition `json:\u0026#34;conditions,omitempty\u0026#34; patchStrategy:\u0026#34;merge\u0026#34; patchMergeKey:\u0026#34;type\u0026#34; protobuf:\u0026#34;bytes,1,rep,name=conditions\u0026#34;` } CRD 매니페스트에 status 서브리소스를 추가하려면, +kubebuilder:subresource:status 마커를 사용해야 합니다.\n// Memcached is the Schema for the memcacheds API //+kubebuilder:subresource:status type Memcached struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec MemcachedSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status MemcachedStatus `json:\u0026#34;status,omitempty\u0026#34;` } Spec은 사용자가 원하는 상태(desired state)를 정의합니다. Status 는 현재의 실제 상태(actual state)를 나타냅니다. 추가한 마커를 통해, 서브리소스를 추가하고, 이를 통해 컨트롤러는 CR 객체의 나머지 부분을 변경하지 않고, status 필드만 업데이트할 수 있습니다.\n참고로 *_types.go 파일을 수정한 후, 해당 리소스 타입의 생성된 코드를 업데이트하기 위해 항상 make generate 명령어를 수행해야 합니다. Makefile 타겟은 controller-gen 유틸리티를 호출하여 api/v1alpha1/zz_generated.deepcopy.go 파일을 업데이트합니다. 이를 통해 모든 Kind 타입이 구현해야 하는 runtime.Object 인터페이스를 우리의 API Go 타입 정의가 구현하도록 보장합니다.\n위의 설명을 좀 더 풀어서 zz_generated.deepcopy.go 파일에 대해 설명하자면, Kubernetes는 컨트롤러, 캐시, 인덱서 등에서 객체를 복사하고 조작합니다. 이 과정에서 안전한 객체 복사(deepcopy)가 필수적입니다. DeepCopyObject() 함수는 runtime.Object 인터페이스를 구현합니다.\ntype Object interface { GetObjectKind() schema.ObjectKind DeepCopyObject() Object } // DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object. func (in *Memcached) DeepCopyObject() runtime.Object { if c := in.DeepCopy(); c != nil { return c } return nil } 이후 make manifests 를 통해 CRD 매니페스트 파일을 생성할 수 있습니다. config/crd/bases/cache.example.com_memcacheds.yaml 경로에 생성됩니다.\nImplement the Controller\ninternal/controllers/memcached_controller.go 파일을 아래 내용으로 교체합니다.\n원 파일\n/* Copyright 2024. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package controller import ( \u0026#34;context\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/client\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/log\u0026#34; cachev1alpha1 \u0026#34;github.com/example/memcached-operator/api/v1alpha1\u0026#34; ) // MemcachedReconciler reconciles a Memcached object type MemcachedReconciler struct { client.Client Scheme *runtime.Scheme } // +kubebuilder:rbac:groups=cache.example.com,resources=memcacheds,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/status,verbs=get;update;patch // +kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/finalizers,verbs=update // Reconcile is part of the main kubernetes reconciliation loop which aims to // move the current state of the cluster closer to the desired state. // TODO(user): Modify the Reconcile function to compare the state specified by // the Memcached object against the actual cluster state, and then // perform operations to make the cluster state reflect the state specified by // the user. // // For more details, check Reconcile and its Result here: // - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.18.4/pkg/reconcile func (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // TODO(user): your logic here return ctrl.Result{}, nil } // SetupWithManager sets up the controller with the Manager. func (r *MemcachedReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;cachev1alpha1.Memcached{}). Complete(r) } https://github.com/operator-framework/operator-sdk/blob/latest/testdata/go/v4/memcached-operator/internal/controller/memcached_controller.go\n이를 원래 파일과 비교하면 내용이 꽤나 많이 추가된 것을 볼 수 있는데, 아래의 내용은 컨트롤러가 어떻게 리소스를 감시하고 reconcile loop 이 작동하는 지 설명합니다.\nSetup a Recorder\nmain.go 에 Recorder: mgr.GetEventRecorderFor(\u0026quot;memcached-controller\u0026quot;), 를 추가합니다.\nif err = (\u0026amp;controller.MemcachedReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), Recorder: mgr.GetEventRecorderFor(\u0026#34;memcached-controller\u0026#34;), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;Memcached\u0026#34;) os.Exit(1) } Reconciler의 개념은 다음과 같습니다. Reconciler는 Kubernetes에서 컨트롤러의 핵심 로직을 담당하는 구조체입니다. Kubernetes에서 컨트롤러는 목표 상태(desired state)와 현재 상태(actual state)를 비교하고, 이를 일치시키는 작업을 수행합니다. Reconciler는 이 과정에서 각 리소스(Custom Resource 포함)를 관찰하고, 필요한 업데이트나 변경 사항을 처리합니다. Event Recorder는 Kubernetes 리소스에서 발생하는 이벤트를 기록하는 역할을 합니다.\nResources watched by the Controller\ncontrollers/memcached_controller.go 파일의 SetupWithManager() 함수는 컨트롤러가 어떻게 구축되어, 해당 CR 및 컨트롤러가 소유하고 관리하는 다른 리소스들을 감시할지를 정의합니다.\nimport ( ... appsv1 \u0026#34;k8s.io/api/apps/v1\u0026#34; ... ) func (r *MemcachedReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;cachev1alpha1.Memcached{}). Owns(\u0026amp;appsv1.Deployment{}). Complete(r) } NewControllerManagedBy() 함수는 다양한 컨트롤러 구성을 가능하게 하는 컨트롤러 빌더를 제공합니다.\nFor(\u0026amp;cachev1alpha1.Memcached{})는 Memcached 타입을 주요 감시 대상 리소스로 지정합니다. 각 Memcached 객체에 대해 Add/Update/Delete 이벤트가 발생할 때마다, 해당 Memcached 객체에 대한 reconcile 요청(네임스페이스/이름 키)이 Reconcile 루프로 전송됩니다.\nOwns(\u0026amp;appsv1.Deployment{})는 Deployment 타입을 보조 감시 대상 리소스로 지정합니다. 각 Deployment 객체에 대해 Add/Update/Delete 이벤트가 발생하면, 이벤트 핸들러는 이를 해당 Deployment의 소유자에 대한 reconcile 요청으로 매핑합니다. 이 경우, Deployment의 소유자는 해당 Memcached 객체입니다.\n이 경우, 의존 객체(Deployment)는 Owner References 필드에 자신의 소유 객체를 참조해야 합니다. 이 필드는 ctrl.SetControllerReference 메서드를 사용하여 추가할 수 있습니다.\n참고: k8s API는 ownerRef 필드에 따라 리소스를 관리합니다. 이 메서드를 사용하면 ownerRef가 올바르게 설정됩니다. 따라서, K8s API는 Memcached Kind의 Custom Resource에 의존하는 리소스(예: Memcached Operand 이미지를 실행하는 Deployment)를 인식하게 됩니다. 이렇게 하면 Custom Resource가 삭제될 때, 모든 의존 리소스도 함께 삭제될 수 있습니다.\nController Configurations\n이외에 컨트롤러를 시작하기 위한 여러 유용한 설정들이 존재합니다. 아래 내용 말고도 builder 와 controller godoc를 통해 다른 설정들을 더 자세하게 살펴볼 수 있습니다.\nconcurrent Reconciles 의 최대 수를 지정하는 옵션은 아래와 같습니다. (기본 값은 1 입니다)\nfunc (r *MemcachedReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;cachev1alpha1.Memcached{}). Owns(\u0026amp;appsv1.Deployment{}). WithOptions(controller.Options{MaxConcurrentReconciles: 2}). Complete(r) } predicates 를 통해 watch events를 필터링 할 수 있습니다.\n감시 이벤트가 Reconcile 루프로의 요청으로 어떻게 변환될지를 결정하려면 적절한 EventHandler 타입을 선택해야 합니다. 기본(primary) 리소스와 보조(secondary) 리소스 관계보다 더 복잡한 Operator 관계에서는, EnqueueRequestsFromMapFunc 핸들러를 사용하여 감시 이벤트를 임의의 Reconcile 요청 세트로 변환할 수 있습니다.\nReconcile loop Reconcile 함수는 감시 중인 CR 또는 리소스에서 이벤트가 발생할 때마다 이 함수가 실행되며, 목표 상태와 현재 상태가 일치하는지 여부에 따라 반환값이 달라집니다.\n이와 같이, 모든 컨트롤러는 Reconcile() 메서드를 구현하는 Reconciler 객체를 가지고 있으며, 이 메서드는 Reconcile 루프를 구현합니다. Reconcile 루프는 Request 인자를 전달받는데, 이 인자는 네임스페이스/이름 키로, 캐시에서 주요 리소스 객체인 Memcached를 조회하는 데 사용됩니다.\nimport ( ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; cachev1alpha1 \u0026#34;github.com/example/memcached-operator/api/v1alpha1\u0026#34; ... ) func (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { // Lookup the Memcached instance for this reconcile request memcached := \u0026amp;cachev1alpha1.Memcached{} err := r.Get(ctx, req.NamespacedName, memcached) ... } Reconciler, Client, 그리고 리소스 이벤트와 상호작용하는 방법에 대한 자세한 안내는 Client API 문서를 참고 하시면 됩니다.\nReconciler에서 사용할 수 있는 반환 옵션 몇 가지는 다음과 같습니다. 더 자세한 것 내용은 Reconcile godoc을 참고하셔야 합니다.\n오류가 있는 경우:\nreturn ctrl.Result{}, err\n오류가 발생하면, ctrl.Result{}와 함께 오류를 반환합니다. 이 경우, Reconcile 함수는 오류를 처리하고 재시도할 수 있습니다.\n오류 없이 재시도 요청:\nreturn ctrl.Result{Requeue: true}, nil\n오류는 없지만, Reconcile 루프를 다시 실행하고 싶을 때 사용합니다. Requeue: true 옵션은 즉시 재시도를 요청합니다.\nReconcile 중지:\nreturn ctrl.Result{}, nil\n리소스의 현재 상태가 목표 상태와 일치할 때 사용합니다. Reconcile 루프를 멈추고, 추가적인 재시도는 요청하지 않습니다.\nX 시간 후에 다시 Reconcile:\nreturn ctrl.Result{RequeueAfter: 5 * time.Minute}, nil\n일정 시간이 지난 후에 Reconcile을 다시 실행하고 싶을 때 사용합니다. 위의 예시에서는 5분 후에 재시도합니다.\nSpecify permissions and generate RBAC manifests\n컨트롤러가 관리하는 리소스와 상호작용하기 위해서는 특정 ****RBAC 권한이 필요합니다. 이러한 권한은 다음과 같은 RBAC 마커(RBAC markers)를 통해 지정할 수 있습니다.\n//+kubebuilder:rbac:groups=cache.example.com,resources=memcacheds,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/status,verbs=get;update;patch //+kubebuilder:rbac:groups=cache.example.com,resources=memcacheds/finalizers,verbs=update //+kubebuilder:rbac:groups=core,resources=events,verbs=create;patch //+kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch func (r *MemcachedReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { ... } make manifests 명령을 통해 ClusterRole 매니페스트를 config/rbac/role.yaml 경로에 생성합니다.\n이후 문서 내용은 배포 과정을 설명하고 있습니다.\n","date":"November 10, 2024","hero":"/images/default-hero.jpg","permalink":"https://kubesy.com/posts/500.-tech_dev/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EB%84%A4%EC%9D%B4%ED%8B%B0%EB%B8%8C-%EA%B0%9C%EB%B0%9C/operator-framework/","summary":"\u003ch2 id=\"개요-\"\u003e개요 :\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://operatorframework.io/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://operatorframework.io/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRed Hat이 주도하는 오픈소스 프로젝트로, 오퍼레이터 개발을 단순화하고 자동화하기 위한 다양한 도구와 라이브러리를 제공합니다. Operator SDK는 Kubebuilder 기반으로 만들어졌지만, Helm, Ansible 등 다양한 언어와 프레임워크를 지원합니다.\n또한 레드햇이 주도하여 개발 중으로 보이는 openstack operator 프로젝트에 사용되었습니다.\u003c/p\u003e\n\u003cp\u003e쿠버네티스의 오퍼레이터와 오퍼레이터 패턴은 공식 문서 \u003ca href=\"https://kubernetes.io/ko/docs/concepts/extend-kubernetes/operator/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://kubernetes.io/ko/docs/concepts/extend-kubernetes/operator/\u003c/a\u003e에 정리되어 있습니다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e오퍼레이터(Operator)는 사용자 정의 리소스를 사용하여 애플리케이션 및 해당 컴포넌트를 관리하는 쿠버네티스의 소프트웨어 익스텐션이다. 오퍼레이터는 쿠버네티스 원칙, 특히 컨트롤 루프를 따른다.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e쿠버네티스 코드 자체를 수정하지 않고 기능을 확대한다는 점에서 큰 이점을 가지고 있습니다. 이미 cncf 의 많은 프로젝트들이 해당 기법을 사용하여 기능을 제공하고 있습니다. CKA 시험에서도 24년 11월 25일 이후부터 시험항목에 관련 내용이 추가됩니다.\u003c/p\u003e","tags":null,"title":"k8s operator 패턴 구현 (Operator Framework)"},{"categories":null,"contents":"쿠버네티스 2 (MSA) 마이크로서비스 아키텍처가 주목받기 이전부터 기업 환경에서는 중복되는 프로세스나 업무들을 하나의 서비스 단위로 개발하여 각 서비스는 호출 가능한 상태로 개발하자는 노력이 계속되어 왔다. 이는 서비스의 생성과 활용을 높여서 비즈니스 환경 변화와 업무 변화에 민첩하게 대응할 수 있는 아키텍처를 갖추기 위함이다(박성훈 2018, 15)\n출처 : 컨테이너 인프라 환경 구축을 위한 쿠버네티스\n코드에 기능 하나를 추가하기 위해서 코드 베이스 전체를 뒤적거리며 일일히 수정하는 경험. 모두들 이런 경험을 해보신 적 있으시지 않으신가요? 비즈니스 환경 변화와 업무 변화에 민첩하게 대응하는 것은 서비스 업계에선 매우 중요한 부분일 것입니다. 하지만 코드베이스가 점점 쌓여 갈수록 작은 변화 하나에도 엄청난 노력을 요하게 됩니다.\n위의 예시는 아래 그림에서 우리가 소위 모놀리식 이라 부르는 아키텍처에서 정말 빈번하게 일어나는 일 입니다. 비즈니스 요구의 변화는 여전히 많은 곳에서 쓰고 있을 워터폴(Waterfall) 개발 프로세스의 관점에선 지옥 그 자체라고 표현할 수 있겠습니다.\n출처 : NHN Cloud\n우리는 지금, 항상 그래왔듯, 빠르게 변화하는 IT 세상에 몸을 맡기고 있습니다. 현재 쪽에 위치한 네 가지 개념 Cloud, DevOps, Microservices, Containers 중 몇은 이전 아티클에서 간략히 소개하거나 설명드렸습니다만, 이번 게시글에서는 Microservice를 자세하게 설명드리고자 합니다.\n마이크로서비스 아키텍쳐 마이크로서비스(microservice)는 애플리케이션을 느슨하게 결합된 서비스의 모임으로 구조화하는 서비스 지향 아키텍처 (SOA) 스타일의 일종인 소프트웨어 개발 기법이다\n마이크로서비스 아키텍처를 소개할 때는 빠짐 없이 등장하는 친구가 하나 있습니다. 바로 SOA입니다. SOA는 1970년대 개념이 언급되었고, 가트너에서 1996년 처음으로 SOA를 소개 하였으며, 2003년 이후 웹 서비스가 구체화되면서 함께 부각 되었습니다. SOAP라는 XML기반의 경량화 된 HTTP 기반 통신 프로토콜을 사용하였고 엔터프라이즈 서비스 버스(ESB)라는 패턴을 사용하여 중앙화된 구성 요소와 백엔드 시스템을 통합한 다음 이를 서비스 인터페이스의 형태로 제공합니다. 따라서 개발자는 기능을 다시 만드는 대신에 기존 기능을 재사용할 수 있습니다.\n미국의 유명 소프트웨어 개발자 Martin Fowler는 SOA를 \u0026ldquo;ServiceOrientedAmbiguity\u0026quot;라고 불렀습니다.\nI\u0026rsquo;ve heard people say the nice thing about SOA is that it separates data from process, that it combines data and process, that it uses web standards, that it\u0026rsquo;s independent of web standards, that it\u0026rsquo;s asynchronous, that it\u0026rsquo;s synchronous, that the synchronicity doesn\u0026rsquo;t matter.…\n서비스 지향 아키텍처(Service-Oriented Architecture, SOA)는 서로 독립적인 기능을 가진 여러 서비스들을 연결하여 하나의 소프트웨어 시스템을 구축하는 아키텍처 스타일입니다. SOA는 서비스의 재사용성, 유연성, 상호운용성, 중앙 집중화된 프레임워크의 부재 등 다양한 장점을 제공합니다. 하지만, 서비스 간의 통신으로 인한 지연 시간, 느슨한 결합을 유지하기 어려운 경우 등의 문제점도 있습니다. (위의 그림에서 수평적으로 길게 늘어진 사각형들이 느슨한 결합을 위협 합니다.)\n그렇기 때문에 SOA 어떻게 보면 철지난 유행처럼 표현되곤 합니다. 하지만 반대로 MSA는 다릅니다. 넷플릭스는 마이크로서비스 아키텍쳐의 가장 좋은 적용 성공 사례라고들 합니다. 국내에서도 최근 PAYCO 쇼핑, 삼성전자, 삼성SDS, 쿠팡, 배달의 민족와 같은 ****수많은 거대 기업들이 MSA화 된 시스템을 구축하기 시작했습니다. 자세한 구현 사례로 11번가의 케이스를 엿볼 수 있습니다.\n마치 코로나 바이러스를 보는 듯 하지만, 아마존과 넷플릭스의 마이크로서비스들을 연결시켜 놓은 그림입니다.\n마치 코로나 바이러스를 보는 듯 하지만, 아마존과 넷플릭스의 마이크로서비스들을 연결시켜 놓은 그림입니다.\nMSA와 SOA 비슷하다면 왜 이리 극명한 온도차를 보이는 걸까요? 우선, MSA와 SOA 모두 비즈니스 문제를 서비스로 분해하는 개념을 중심으로 합니다. 허나 SOA 는 모듈의 의존성은 줄이되 모듈 내에서 공유할 수 있는건 최대한 공유하는 정책을 사용하고, MSA 는 가능한 공유하지 않고 모듈들이 독립적으로 운용될 수 있도록 아키텍처를 디자인 합니다.\n기존 서비스 지향 아키텍처(SOA)는 서비스 자체가 아닌 서비스 버스에 비즈니스 로직이 점점 더 추가되므로 서비스에 응집력이 부족한 경우가 많습니다. 응집도라는 단어가 몹시 추상적일 수 있겠지만 아래 그림과 함께 살펴보면 이해에 도움이 될 수 있겠습니다.\n응집도(cohesion)는 마이크로서비스를 쪼갤지, 그룹화 할지를 구분할 수 있는 방법 중 하나 입니다. 응집도가 클수록 결합도(coupling)가 낮아집니다. 다시 말해 결합도는 낮을 수록 응집도는 높을 수록 이상적인 모듈화이라고 할 수 있겠습니다. SOA의 그림을 살펴봅시다. 한 눈에 보아도 공통 영역이 아주 넓게 형성되어 있음을 확인할 수 있습니다. MSA는 서비스가 공유되기보다 독립적으로 실행되는 것을 지향합니다. 모놀리식(통짜)으로 구성된 서비스를 도메인 별로 잘게 잘게 쪼게 하나의 독립적인 로직을 수행할 수 있게 됩니다. 그렇다고 서비스를 무조건 작은 단위-예를들면 CRUB 수준으로 쪼개어, 운영 조차 하기 힘들 정도가 되어선 안됩니다. 서비스를 나누되 결제, 예약, 정산 등과 같이 적절한 경계를 만들어야 합니다.\n마이크로서비스 아키텍쳐가 유행이라면 왠지 모르게, 모놀리식 아키텍쳐가 마치 또 다른 철지난 유행이되야 할 것만 같은 느낌을 줍니다만, 이에 반하는 의견을 피력하는 사람도 분명 존재 합니다. MSA 구현에도 역시 몇몇 단점이 존재합니다. 특히 무수히 많아진 장애 가능 지점에 관한 부분이 대표적입니다.\n만약 MSA가 괜찮아보이고, MSA로 비즈니스 서비스를 구축하기로 결심했다면, MSA 구현에 필요한 다섯가지의 핵심 원칙을 마음에 새겨야 합니다.\nAutonomy (자율성) : 서비스는 느슨하게 결합 해야 하고 독립적으로 배포 가능 해야 합니다.\nResilience (회복력) : 서비스를 무수히 쪼개다보면 장애 가능 지점이 증가하게 됩니다. 장애가 생겼을 때 서킷브레이커나 타임아웃등을 적용하여 적절하게 에러를 처리해야 합니다.\nTransparency(투명성) : 장애 지점을 재빠르게 찾아내기 위해서는, 전체 서비스 구조를 명확하게 파악할 수 있어야 합니다.\nAutomation(자동화) : 서비스를 일일히 수동으로 테스트하고 배포하는 일은 불가능에 가깝습니다. DevOps 시스템을 구축해야 합니다.\nAlignment(정렬) : 서비스와 팀을 구성하는 데 가장 중요한 요소는 비즈니스 컨셉입니다.\nMSA기반 개발 환경에는 역 콘웨이의 법칙이 적용 되어야 합니다. 콘웨이의 법칙이란 애플리케이션 아키텍처는 그것을 개발하는 조직의 구조를 그대로 반영한다는 뜻입니다. 따라서 이 법칙을 역으로 이용해서 조직의 구조가 마이크로서비스 아키텍처에 고스란히 반영되도록 설계해야 합니다. 이렇게 하면 개발 팀과 서비스를 느슨하게 결합시킬 수 있습니다.\n또한 MSA에는 다양한 패턴들이 존재합니다. 아래의 패턴들에 대해 알고 있는 것은 효과적인 설계를 위해선 매우 중요한 부분입니다.\nCommunication Patterns Synchronous\t(RESTful, gRPC, GraphQL, WebSocket) Asynchronous Messaging (AMQP, Kafka, NATS) Connectivity and Composition Patterns Service Mesh Service Choreography Saga Data Management Patterns DB (RDB, NoSQL) Data Management Data Scaling (Sharding, CQRS) Event-Driven Architecture Patterns Event-Delivery State Management Orchestration Stream-Processing Patterns Streaming Data Processing Scaling and Performance Optimization Reliability API Management and Consumption Patterns API Gateway 위의 패턴들에 대해 더 자세하게 알고 싶다면 다음의 책을 추천합니다.\nDesign Patterns for Cloud Native Applications (O\u0026rsquo;Reilly Media, Inc., 2021. 314 p. ISBN: 978-1-492-09071-7.)\n여기까지 마이크로서비스 아키텍처(MSA)에 대한 개념과 SOA와의 차이점, MSA의 장단점, MSA 구현에 필요한 핵심 원리, 그리고 다양한 MSA 패턴들에 대해 다루어 보았습니다. 이 정도면 마이크로서비스 아키텍쳐에 대한 간략한 소개가 된 것 같습니다.\n하지만 중요한 것은 쿠버네티스가 MSA 환경에서 어떤 역할을 하는지 입니다. 저번 아티클의 가장 마지막 질문을 기억하실 지 모르겠습니다.\n\u0026ldquo;컨테이너를 그렇게 많이 구동 시킬 필요가 있나? 데이터베이스든 운영체제든 아무리 많이 필요하다고 해도 손가락으로 셀 수 있을 정도 아닐까?\u0026rdquo;\n이에 대한 답을 내려보자면, 모놀리식을 고수하는 개발팀은 굳이 그럴 필요는 없겠습니다. 그러나 MSA 개발팀이라면 필수불가결할 듯 합니다. (인정하지 못하시겠다면, 다시 한 번 보고 옵시다) MSA의 서비스 하나 하나 (각각 Endpoint를 가지는, 예를 들어 서버)들은 결국 깡통 서버가 되든지, VM이 되든지, Container로 올리던지 해야할 테니까요. (이 중 무엇을 사용하는 게 가장 편한 길일까요?)\n결국 쿠버네티스는 마이크로서비스 환경에서 서비스의 배포, 확장 및 관리를 담당하는 컨테이너 오케스트레이션 도구입니다. 이를 통해 개발자들은 더욱 빠르게 안정적인 서비스를 제공할 수 있으며, 유지보수 및 확장성도 용이해집니다. 즉, 쿠버네티스는 마이크로서비스 아키텍처에서 필수적인 도구 중 하나입니다.\n자, 저희는 아주 먼 길을 돌고 돌아온 끝에 쿠버네티스에 대해 얼추 감을 잡은 것 같습니다. 다음 아티클은 Kubernetes의 자세한 구조와 구성요소에 대한 글이 될 것 같습니다.\n참고자료\nMicroservices in Action (ISBN 978161729445)\nhttps://dzone.com/articles/service-oriented-architecture-a-dead-simple-explan\nhttps://wiki.webnori.com/display/devbegin/SOA+VS+MSA\nhttps://always-kimkim.tistory.com/entry/SOA와-MSA의-차이점\n","date":"February 23, 2023","hero":"/images/hero/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EA%B0%80%20%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C.png","permalink":"https://kubesy.com/posts/500.-tech_dev/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EA%B8%B0%EB%B3%B8/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EA%B0%80-%EB%AD%90%EC%A7%802/","summary":"\u003ch3 id=\"쿠버네티스-2-msa\"\u003e쿠버네티스 2 (MSA)\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e마이크로서비스 아키텍처가 주목받기 이전부터 기업 환경에서는 중복되는 프로세스나 업무들을 하나의 서비스 단위로 개발하여 각 서비스는 호출 가능한 상태로 개발하자는 노력이 계속되어 왔다. 이는 서비스의 생성과 활용을 높여서 비즈니스 환경 변화와 업무 변화에 민첩하게 대응할 수 있는 아키텍처를 갖추기 위함이다(박성훈 2018, 15)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"/posts/images/%ec%bf%a0%eb%b2%84%eb%84%a4%ed%8b%b0%ec%8a%a4%ea%b0%80%20%eb%ac%b4%ec%97%87%ec%9d%bc%ea%b9%8c/2_0.png\"\n    \n    \n    \n    \n    \n        class=\"center\"\n    \n\u003e\n\n출처 : \u003cstrong\u003e\u003ca href=\"https://www.bing.com/ck/a?!\u0026amp;\u0026amp;p=9ca213ca31e4da74JmltdHM9MTY3NzExMDQwMCZpZ3VpZD0zODY4NDk3ZS04NmIyLTY5MzUtMjYwMC01OGUwODc5YTY4YTkmaW5zaWQ9NTE3OA\u0026amp;ptn=3\u0026amp;hsh=3\u0026amp;fclid=3868497e-86b2-6935-2600-58e0879a68a9\u0026amp;psq=%ec%bf%a0%eb%b2%84%eb%84%a4%ed%8b%b0%ec%8a%a4\u0026#43;%eb%8f%84%ec%bb%a4\u0026#43;%ec%b1%85\u0026amp;u=a1aHR0cDovL3d3dy55ZXMyNC5jb20vUHJvZHVjdC9Hb29kcy8xMDIwOTk0MTQ\u0026amp;ntb=1\" target=\"_blank\" rel=\"noopener\"\u003e컨테이너 인프라 환경 구축을 위한 쿠버네티스\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e코드에 기능 하나를 추가하기 위해서 코드 베이스 전체를 뒤적거리며 일일히 수정하는 경험. 모두들 이런 경험을 해보신 적 있으시지 않으신가요?  비즈니스 환경 변화와 업무 변화에 민첩하게 대응하는 것은 서비스 업계에선 매우 중요한 부분일 것입니다.  하지만 코드베이스가 점점 쌓여 갈수록 작은 변화 하나에도 엄청난 노력을 요하게 됩니다.\u003c/p\u003e","tags":null,"title":"쿠버네티스가 뭐지 2"},{"categories":null,"contents":"22년도에 작성했던, 일반인을 위한 쿠버네티스 소개글입니다\u0026hellip; 배운지 몇달 안 된 상태에서 썼던 글이라 각종 (이상한) 추상적인 표현이 많습니다.\n안녕하세요. 좋은 기회를 통해 최근 공부하고 있는 IT 기술인 쿠버네티스에 대해 소개 해드리고자 합니다.\n쿠버네티스? 최근 NHN사의 클라우드 교육을 다녀왔었습니다. 클라우드는 이제 많은 사람이 쉽고 편하게 사용하는 기술이지요? 매킨토시를 쓰거나, 윈도우를 쓸 때 icloud, OneDrive등의 기능을 쓰시는 분이 많을테고, 구글에서도 구글 드라이브에 자료를 저장하는 등의 경험이 있으실 것 같습니다. 그런 클라우드 시장에서 주목하는 기술 중 하나가 쿠버네티스 입니다.\n쿠버네티스에 대해 처음 들으시는 분들이 계실 것 같습니다. 저도 몇 달 전만 하더라도 일전에 한 번도 들어본 적 없는 이름이었습니다. 그래서 일까요? 왠지 신기술 처럼 느껴졌었네요. 하지만 쿠버네티스가 유명해 진 것이 최근이라고 하더라도 따끈따끈한 존재라기엔 나이가 꽤나 있습니다. 무려 8년 전인 2014년에 등장한 존재이지요. 구글과 리눅스 재단이 설립한 CNCF(Cloud Native Computing Foundation)에서 공식으로 쿠버네티스를 오픈소스화 하였습니다. 지금까지 지속적으로 업데이트 되고 있구요. 이미 기술적으로 성숙화 단계에 있다고들 하네요.\n쿠버네티스는 \u0026lsquo;가상화\u0026rsquo;나 \u0026lsquo;컨테이너\u0026rsquo; 혹은 \u0026lsquo;도커\u0026rsquo;와 \u0026lsquo;MSA(마이크로서비스 아키텍쳐)\u0026rsquo; 등의 키워드와 아주 밀접한 관계에 있습니다. 그렇기에 클라우드 환경과 아주 어울리는 기술이 되고 많은 이들의 주요한 관심을 끌고 있지요. 현재 클라우드 환경과 마이크로서비스 아키텍쳐가 가지는 장점과 매력은 세간에서 각광받는 키워드 일 것 같습니다. 아마 여러분 중에서도 마이크로서비스 아키텍쳐에 대해 이미 들어보신 분도 있을테고 실제로 활용해보신 분도 계실 것 같습니다. MSA를 채택한 가장 유명한 회사로 넷플릭스가 있습니다.\n혹시, 위에 언급된 여러 키워드 중에, 얼핏 들어보았는데 생소하거나 아니면 무슨 이야기를 하려는 지 전혀 감이 오지 않으시는 분 있으실까요? 정말 다행이네요. 이 글은 쿠버네티스를 처음 들어보시는 분들을 위해 약간의 도움을 드리고자 작성되었습니다. 한 번쯤 스쳐 지나가면서 보면 좋겠다 싶은 느낌으로요! (솔직히 쿠버네티스를 접한지 얼마되지 않았기 때문에 아주 깊은 수준의 기술을 설명해드릴 자신이 없네요 😅 )\n쿠버네티스에 대해 처음 들어보시는 분들께 제가 알고 있는 한에서 최대한 쉽게, 무엇을 위한 기술인지 설명 드리고자 합니다. 또한 어떤 이유로 쿠버네티스를 공부하고 있는지, 이것으로 뭘하고 있는지, 뭘 하려고 하는지 설명드릴 수 있는 기회가 되었으면 좋겠습니다.\n천천히 개념 접근하기 우선 쿠버네티스라는 말 자체와 친해져볼까요? Kubernetes란 명칭은 키잡이(helmsman)나 파일럿을 뜻하는 그리스어입니다. IT 기술 용어치고는 꽤나 특이한 의미를 가지고 있네요. 일종의 상징이나 비유 같은 걸까요? 실제로 쿠버네티스 로고 이미지는 배에서 사용할 법한 키로 되어있습니다. 심지어 바다를 떠올리게 하는 푸른색으로 뒤덮혀있지요.\n우리는 배 위에서 키를 잡는 사람을 선장이라 부릅니다. 선장은 바다 위를 누비며 자신을 따르는 선원들을 지휘하고 관리합니다. 영화 캐리비안의 해적에서 잭 스패로우가 그랬듯이요. 선장은 간단한 표현으로 배 위의 모든 것을 운영을 하는 사람입니다. 쿠버네티스의 로고가 \u0026lsquo;키\u0026rsquo;로 만들어진 이유가 바로 이것입니다. 모든 리더가 그러하듯, 키잡이(쿠버네티스)의 핵심적인 요소가 바로 \u0026lsquo;운영\u0026rsquo; 혹은 \u0026lsquo;관리\u0026rsquo;거 든요.\n구체적으로 무엇을 운영 혹은 관리 하는 것 일까요? 먼저 잭 스페로우가 뭘 했는지 부터 되새겨 봅시다.\n캐리비안의 해적 1편의 메인 소재였던 수많은 금화가 담긴 거대한 (저주 받은) 보물 상자를 기억하시나요? 잭 스패로우를 포함한 당대 해적, 그 중 선장들은 선원들을 부리며 단 하나의 목표에 사로 잡혀 있었습니다. 바로 보물을 찾아 영원한 명성과 부귀영화를 누리는 것 입니다. 보물 상자가 배 위에 올라온 순간 키잡이에게는 하나의 임무가 생깁니다. 육지까지 안전하게 보물을 운송하는 것입니다.\n위험의 요소는 참으로 많습니다. 다른 해적들에게서 지켜내는 게 최우선이구요. 또한 내부적으로 보물 상자를 훔쳐 달아나려는 선원도 관리해야 하고요. 또는 이외에 기이한 날씨 같이 기상천외한 현상으로부터 항해를 방해는 요소도 있었지요. 영화를 보면서 느낀 점이지만, 저런 환경에서는 참 살기 힘들었을 것 같아요.\n평화의 시대인 오늘날의 키잡이들은 상황이 좀 더 낫습니다. 과거보다 위험의 요소가 더 적다고 할까요? 단적으로 아래 사진처럼 모두가 탐내던 배 위의 귀중한 \u0026lsquo;보물 상자\u0026rsquo;는 수많은 \u0026lsquo;컨테이너 상자\u0026rsquo;로 대체 되었습니다. 여전히 바다 위로 선박을 노리는 해적들이 간간히 나타난다지만, 아주 극소수의 해프닝이 되었네요 (허나 무서운 일이지요). 저는 이분들 덕분에 아마존에서 주문한 해외 스타워즈 굿즈가 컨테이너 속에 담겨 안전하게 배송되어 너무나 행복합니다.\nIT기술을 설명한다면서 웬 뚱딴지 같은 이야기인가 싶으시겠지요? 너그러운 마음으로 이해를 돕기위한 긴 준비 단계였다고 생각해주세요. 사실은요, IT기술 쿠버네티스도 위의 예시들과 별반 다르지 않습니다.\n쿠버네티스는 컨테이너를 관리 운영하는 데 사용합니다. 그래서 컨테이너 오케스트레이션(Container Orchestration) 도구라고도 불리지요. 컴퓨터 세상에도 여러가지 위험요소 같은 것들이 많잖아요? 예를 들어 프로그램 켜놓고 까먹는다 던가, 필요한 파일을 필요할 때 못 찾는다던가 하는 느낌으로요. 컨테이너를 관리하는 데도 비슷한 일이 있는데요 …\n잠깐, 제가 아는 그 컨테이너가 맞아요? 아, 컨테이너가 뭔지부터 설명을 드려야 겠네요.\n위의 사진 같이 실생활 용품 가득 넣어진 화물 컨테이너를 생각하신다면, 아닙니다. 만약 그런거라면 쿠버네티스는 해운사에서 사용하는 프로그램쯤 되려나요? 😁\nIT 세계에서 언급되는 컨테이너는 하나의 \u0026lsquo;운영 체제 커널\u0026rsquo;에서 다른 프로세스에 영향을 받지 않고 \u0026lsquo;독립적으로 실행되는 프로세스\u0026rsquo; 상태를 의미 합니다. 제 표현이긴 하지만 컴퓨터 세상의 화물 컨테이너라고 해도 무관해보여요. 실제로 컨셉을 저기서 많이 따온 것 이구요.\n어떤 컨셉인지 화물 컨테이너와의 연관점으로 쉽게 이해해 봅시다. 우리가 사진으로 보듯 화물 컨테이너 외부는 다들 색깔만 다르지 비슷한 모양과 크기로 쌓여져 있습니다. 여러 화물 컨테이너가 같은 선박 위에 놓여있는 상황은 여러 인스턴스들이 \u0026lsquo;하나의 운영 체제 커널 위\u0026rsquo;에 있는 것과 비슷합니다.\n또, 중요한 것은 내부입니다. 화물 컨테이너 겉만 봐서는 내부에 어떤 물건이 놓여있는 지는 전혀 감을 잡을 수가 없습니다. 또 외부의 환경으로부터 대체로 분리되어 있고요. \u0026lsquo;독립적으로 실행되는 프로세스\u0026rsquo;라는 것이 바로 외부로 부터 지켜지는 \u0026lsquo;내부의 스타워즈 굿즈\u0026rsquo;와 같은 개념 입니다.\n정말 컴퓨터 세계에서 사용하는 화물 컨테이너라고 할 수 있겠죠? 다만, 내부에 좀 컴퓨터 적인 친구들을 담고 있을 뿐이죠. 가령 우분투나 센토스 같은 리눅스 운영체제 이미지 같은 거 말이에요. 컨테이너를 실행(컴퓨터 세상이다보니까 운송하는 대신 실행합니다)하는 외부의 환경이 윈도우 운영체제라도 문제 없습니다. 내부의 물건은 외부로부터 지켜지고 있거든요!\n컨테이너 기술은 가상 머신(Virtual Machine)과 많이 비교되곤 합니다. 둘의 공통점은 가상화 기술을 사용한다는 것 입니다. 가상화는 단일한 물리 하드웨어 시스템에서 여러 시뮬레이션 환경이나 전용 리소스를 생성할 수 있는 기술입니다. 구체적인 건 몰라도, 컨테이너 기술은 가상 머신에 비해 무척 가볍게 실행 됩니다. 가상 머신에서는 하이퍼바이저라는 것을 이용해 새로운 운영체제를 실행해야 하는데, 컨테이너는 그럴 필요가 없습니다. 그렇기에 똑같은 우분투 버전을 설치한다고 했을 때 용량 측면에서도, 실행 속도 측면에서도 꽤나 비교됩니다.\n컨테이너를 활용하는 경우에 대한 예제를 설명 드려볼까요? 저는 학창 시절에 \u0026lsquo;포트란\u0026rsquo;이라는 프로그래밍 언어를 사용했습니다. 포트란의 컴파일러 중 하나인 gfortran은 리눅스 환경에서는 무료로 쉽게 설치가 가능합니다. 하지만 리눅스에 대해 일도 몰랐던 저는 처음엔 윈도우 환경에서 포트란을 사용하고자 했었죠. 하지만 곧 마음을 바꿔 먹어야 했습니다. 윈도우에서 포트란을 사용하는 방법이 아예 없던 것은 아니지만, 리눅스 환경보다는 설치가 좀 더 어려웠고, 무료버전 컴파일러가 깔끔하지도 않았거든요. 저는 결국 리눅스와 친해지기로 결심하고 우분투를 컴퓨터에 설치했는데요. 은행 업무나 아래한글 프로그램 같은 것을 활용하기에 참 힘든 환경이었기에 꽤나 고생했던 기억이 납니다. 그런데 지금와서 생각해보면 쉽게 해결될 문제였습니다. 윈도우 운영체제 위에서 컨테이너 기술을 활용해 리눅스를 구동시켰다면 말이에요. (질문 - 컨테이너를 VM 처럼 써도 되는 걸 까요?)\n또 다른 예시는 데이터베이스 설치입니다. 데이터베이스도 컨테이너 환경으로 쉽게 구동 할 수 있습니다. 사용하지 않을 때는 컨테이너를 내려 마치 컴퓨터에 없는 존재처럼 만들 수도 있지요. 아까 컨테이너 내부는 외부로부터 지켜진다고 하였는데, 실제로 기본적인 설정으로는(네트워크 설정을 따로 해주지 않으면) 외부에서 컨테이너 내부로 통신할 방법이 없습니다. 컨테이너 내부에 독자적인 가상 네트워크만이 설정되기 때문이죠. 그럼 데이터베이스를 사용할 방법이 없게 느껴지시겠지만, 이제 쿠버네티스(혹은 도커)가 이런 부분을 해결해 줄 수 있습니다.\n도커? 쿠버네티스랑 같은 거에요? 도커랑 쿠버네티스는 다른 것입니다. 그러나 뗄 수 없는 사이 입니다.\nDocker라는 영단어는 사전적으로 항만 노동자라는 의미를 가지고 있습니다. 쿠버네티스와 비슷하게 바다를 연상케 하는 단어지요. 저는 위에서 여러번 컨테이너를 \u0026lsquo;실행\u0026rsquo;한다 라는 표현을 사용 했습니다. 도커라는 녀석은 이를 가능케 해주는 오픈 소스 프로젝트 입니다. 쿠버네티스를 사용하기 위해서는 도커와 같은 \u0026lsquo;컨테이너 런타임\u0026rsquo; 기술을 필요로 합니다.\n(정확하게는 도커와 쿠버네티스가 아니라, 컨테이너 런타임과 쿠버네티스는 뗄레야 뗄 수 없는 사이라고 해야겠습니다)\n로고를 한 번 보고 가시죠. 귀엽게 생긴 고래 배가 수 많은 컨테이너를 실고 물 위에 떠 있습니다. 그림이 암시하는 것처럼 사실 쿠버네티스 없이 도커만 있어도 컨테이너 한두 개를 운영하는 것은 문제가 되지 않습니다.\n허나, 도커가 출시된 해는 2013년이고 쿠버네티스보다 1년 정도 빠르게 등장한 기술입니다. 도커가 컨테이너 환경을 이용할 완벽한 기술이라면 쿠버네티스가 등장할 필요가 있었을까요? 쿠버네티스가 뒤늦게 등장했다는 것은 도커만으로 해결할 수 없는 문제가 있지 않을까라는 추론을 가능케 합니다. 네, 도커에는 중대한 문제가 있습니다. 한두 개가 아닌 수많은 컨테이너를 관리하기 힘들다라는 것 입니다.\n아마, 이런 생각을 하실 수 있으실 것 같습니다. 컨테이너를 그렇게 많이 구동 시킬 필요가 있나? 데이터베이스든 운영체제든 아무리 많이 필요하다고 해도 손가락으로 셀 수 있을 정도 아닐까? 이 질문에 대한 답이 바로 MSA(마이크로서비스 아키텍쳐)입니다.\n(다음에 계속)\n","date":"September 11, 2022","hero":"/images/hero/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EA%B0%80%20%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C.png","permalink":"https://kubesy.com/posts/500.-tech_dev/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EA%B8%B0%EB%B3%B8/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EA%B0%80-%EB%AD%90%EC%A7%80/","summary":"\u003cp\u003e22년도에 작성했던, 일반인을 위한 쿠버네티스 소개글입니다\u0026hellip; 배운지 몇달 안 된 상태에서 썼던 글이라 각종 (이상한) 추상적인 표현이 많습니다.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e안녕하세요.\n좋은 기회를 통해 최근 공부하고 있는 IT 기술인 쿠버네티스에 대해 소개 해드리고자 합니다.\u003c/p\u003e\n\u003ch3 id=\"쿠버네티스\"\u003e쿠버네티스?\u003c/h3\u003e\n\u003cp\u003e최근 NHN사의 클라우드 교육을 다녀왔었습니다. 클라우드는 이제 많은 사람이 쉽고 편하게 사용하는 기술이지요? 매킨토시를 쓰거나, 윈도우를 쓸 때 icloud,  OneDrive등의 기능을 쓰시는 분이 많을테고, 구글에서도 구글 드라이브에 자료를 저장하는 등의 경험이 있으실 것 같습니다.  그런 클라우드 시장에서 주목하는 기술 중 하나가 쿠버네티스 입니다.\u003c/p\u003e","tags":null,"title":"쿠버네티스가 뭐지 1"},{"categories":null,"contents":"Harbor 참고자료\nhttps://happycloud-lee.tistory.com/165\nhttps://goharbor.io/docs/2.1.0/install-config/\n쿠버네티스에서 파드를 생성할 때 필수적으로 필요로 하는 것이 바로 컨테이너 이미지 입니다. 컨테이너 이미지는 로컬에 저장할 수도 있고 도커 허브와 같은 원격 환경을 이용할 수도 있습니다. 원격 환경이 다른 사람들에게 이미지를 공유하거나 할 때 편리하지만, 문제는 도커 허브 이용에 제한이 생겼다는 것 입니다. 특정 시간 동안 특정 횟수의 다운로드 제한이 있습니다.\n유료 모델을 사용하는 것도 하나의 방법이지만, 그보다 무료 이미지 저장소를 로컬에 설치 하는 것이 더 바람직할 듯 합니다. Harbor는 그 목적에 아주 적합 합니다.\nInstallation 쿠버네티스에서 HA하게 하버를 이용하기 위해, Helm을 통해 설치하는 것 권장하고 있습니다.\n미리 준비 해두어야 할 조건은 다음과 같습니다.\nKubernetes cluster 1.10+ ⇒ 이번 테스트에서는 1.27 버전을 사용하고 있습니다. Helm 2.8.0+ ⇒ Helm 3을 사용하고 있습니다. High available ingress controller (Harbor does not manage the external endpoint) ⇒ 노드 포트를 사용할 수 있습니다. nginx 인그레스 컨트롤러 앞에 proxy를 붙혔을 때 왠지 모를 ssl 인증오류가 생겨서, 노드 포트를 열고 이를 proxy에 등록해줄 것 입니다. High available PostgreSQL 9.6+ (Harbor does not handle the deployment of HA of database) ⇒ 파드로 만들어 놓아야 하는 걸까요? High available Redis (Harbor does not handle the deployment of HA of Redis) ⇒ 위와 동일한 질문이 있습니다. 답을 먼저 말씀 드리자면, 꼭 미리 준비해 둘 필요는 없습니다. PVC that can be shared across nodes or external object storage ⇒ 준비 된 PVC가 필요한데, 저희는 동적 프로비저닝 기능을 사용할 것입니다. # 헬름 리포를 등록하고, 폴더를 내려 받습니다. helm repo add harbor https://helm.goharbor.io helm fetch harbor/harbor --untar 명령어 실행한 경로에 /harbor 폴더가 생성되어 있습니다.\n# cd ./harbor \u0026amp;\u0026amp; ls -al rwxr-xr-x 4 root root 123 7월 31 18:03 . dr-xr-x---. 64 root root 4096 7월 31 18:03 .. -rw-r--r-- 1 root root 57 7월 31 18:03 .helmignore -rw-r--r-- 1 root root 567 7월 31 18:03 Chart.yaml -rw-r--r-- 1 root root 11357 7월 31 18:03 LICENSE -rw-r--r-- 1 root root 192242 7월 31 18:03 README.md drwxr-xr-x 2 root root 58 7월 31 18:03 conf drwxr-xr-x 15 root root 4096 7월 31 18:03 templates -rw-r--r-- 1 root root 33874 7월 31 18:03 values.yaml values.yaml 파일을 수정해야 합니다. Ingress rule, External URL, External PostgreSQL, External Redis, Storage 부분들에 변화가 필요하다고 합니다. 허나, 위에서 언급했듯 저는 PostgreSQL와 Redis도 내부 저장소 사용할 예정입니다. 결국 따로 필요로 하는 것은 Storage 설정입니다.\nDynamic provisioning : pvc with NFS 기본적인 PV / PVC 사용 순서는 미리 PV를 만들어 놓고, \u0026ldquo;저 PV를 쓸거야\u0026quot;라는 선언의 목적으로 PVC를 만들어 놓아야 합니다. 하지만 동적 프로비저닝을 사용하면 PVC 하나만 적용시켜도 자동으로 PV가 생성되게 됩니다. 참고로 설치에 사용한 볼륨은 13G 정도 입니다.\nNFS 서버 시작하기\n# nfs-server를 다운 받습니다. systemctl start nfs-server systemctl enable nfs-server # nfs 저장소로 사용할 폴더를 생성해 줍니다. # 경로는 자유입니다. 저는 nfs 폴더를 생성하였습니다. mkdir nfs # /etc/exports 파일에 경로를 등록합니다. # [path] = 파일 경로 # [cidr] = ex) 192.168.15.0/24 echo \u0026#39;[path] [cidr](rw,sync,no_root_squash)\u0026#39; \u0026gt;\u0026gt; /etc/exports #설정을 적용하고 결과를 확인합니다. exportfs -r showmount -e 동적 프로비저닝 설정하기\n참고자료 → Kubernetes k8s Volume 동적 프로비저닝 with NFS 기본 스토리지 클래스\n⇒ 쿠버네티스 노드 모두에 nfs 클라이언트를 받아놓아야합니다. 위의 블로그에 과정이 자세하게 나와 있습니다. 굳이 디폴트 스토리지 클래스 까지 등록해주실 필요는 없습니다.\n궁금한 점 중 하나는 깃허브 사이트에 보면 헬름으로도 이를 만들어줄 수 있는 듯 한데, 만약 여러 경로를 사용하려면, 여러 번 헬름 install을 수행하면 되는 것인가,, 라는 생각이 듭니다.\nvalues.yaml /harbor 경로에 있는 폴더에 들어가보면 values.yaml 이라는 파일이 존재합니다. 하버 뿐만이 아니라, 여러 헬름 프로젝트에서는 설치에 필요한 각종 정보들은 위의 파일로 관리 합니다.\n파일을 열어보면 1000줄 가까이의 무지막지한 라인들이 저를 반겨줍니다. 이 설정들을 하나하나 뜯어봐야하는 공포에 사로잡히지만, 다행이 수정할 부분은 그리 많지 않습니다.\n가장 먼저 expose.type을 nodePort로 바꿔봅시다. 인그레스를 활용한 케이스는 이 블로그를 살펴보시면 좋을 것 같습니다. 저도 처음에 nginx ingress 컨트롤러에 연결해 보았다가, 프록시와의 이유 모를 통신 이슈로 nodePort로 선회 했습니다.\n인그레스 이외의 설정(\u0026ldquo;clusterIP\u0026rdquo;, \u0026ldquo;nodePort\u0026rdquo; or \u0026ldquo;loadBalancer\u0026rdquo;)을 사용한다면, tls 인증을 위해 expose.tls.auto.commonName을 입력해 줍시다. 저는 프록시에서 사용할 도메인을 입력해 주었습니다. tls 인증이 harbor CA 라는 곳에서 이뤄지던데, 정확이 어떤 곳인지는 잘 모르겠습니다.\n또한 externalURL 쪽에도 https://[domain]을 입력해 주었습니다.\n그 다음 볼륨 설정이 필요합니다. persistence.persistentVolumeClaim 쪽에서 registry, jobservice, database, redis, trivy 부분 아래 공통적으로 storageClass을 동적 프로비저닝 스토리지의 클래스로 지정해줘야 합니다.\n설정이 끝났습니다.\nHelm install helm을 이용한 설치는 언제나 편리합니다. 다만, 명령어 실행 부분이 harbor 폴더를 바라보고 있어야 합니다.\n# 네임스페이스 생성 kubectl create namespace harbor # harbor 네임스페이스에 harbor 폴더의 설정 파일을 이용해 harbor를 설치 helm install -n harbor harbor harbor/ 설정이 바뀌었을 때 적용 방법입니다.\nhelm upgrade -n harbor harbor harbor/ 설치를 확인 해봅시다.\n$ k get all,pv,pvc -n harbor NAME READY STATUS RESTARTS AGE pod/harbor-core-7db5b74c86-nctk2 1/1 Running 0 148m pod/harbor-database-0 1/1 Running 0 15h pod/harbor-jobservice-865b66687b-mg9dr 1/1 Running 3 (148m ago) 148m pod/harbor-nginx-5cb465f58f-fmmrz 1/1 Running 0 148m pod/harbor-notary-server-854944cd7d-kvsnf 1/1 Running 1 (143m ago) 148m pod/harbor-notary-signer-76b6978d97-h7gfl 1/1 Running 0 148m pod/harbor-portal-d7dbcc46c-94rxr 1/1 Running 0 15h pod/harbor-redis-0 1/1 Running 0 15h pod/harbor-registry-d7c5dbf6f-7pzxc 2/2 Running 0 148m pod/harbor-trivy-0 1/1 Running 0 15h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/harbor NodePort 10.96.115.165 \u0026lt;none\u0026gt; 80:30002/TCP,443:30003/TCP,4443:30004/TCP 148m service/harbor-core ClusterIP 10.96.117.194 \u0026lt;none\u0026gt; 80/TCP 15h service/harbor-database ClusterIP 10.104.0.73 \u0026lt;none\u0026gt; 5432/TCP 15h service/harbor-jobservice ClusterIP 10.102.37.222 \u0026lt;none\u0026gt; 80/TCP 15h service/harbor-notary-server ClusterIP 10.108.64.69 \u0026lt;none\u0026gt; 4443/TCP 15h service/harbor-notary-signer ClusterIP 10.111.161.104 \u0026lt;none\u0026gt; 7899/TCP 15h service/harbor-portal ClusterIP 10.101.185.44 \u0026lt;none\u0026gt; 80/TCP 15h service/harbor-redis ClusterIP 10.98.124.114 \u0026lt;none\u0026gt; 6379/TCP 15h service/harbor-registry ClusterIP 10.100.29.39 \u0026lt;none\u0026gt; 5000/TCP,8080/TCP 15h service/harbor-trivy ClusterIP 10.107.210.23 \u0026lt;none\u0026gt; 8080/TCP 15h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/harbor-core 1/1 1 1 15h deployment.apps/harbor-jobservice 1/1 1 1 15h deployment.apps/harbor-nginx 1/1 1 1 148m deployment.apps/harbor-notary-server 1/1 1 1 15h deployment.apps/harbor-notary-signer 1/1 1 1 15h deployment.apps/harbor-portal 1/1 1 1 15h deployment.apps/harbor-registry 1/1 1 1 15h NAME DESIRED CURRENT READY AGE replicaset.apps/harbor-core-7db5b74c86 1 1 1 148m replicaset.apps/harbor-jobservice-865b66687b 1 1 1 148m replicaset.apps/harbor-nginx-5cb465f58f 1 1 1 148m replicaset.apps/harbor-notary-server-854944cd7d 1 1 1 148m replicaset.apps/harbor-notary-signer-76b6978d97 1 1 1 148m replicaset.apps/harbor-portal-d7dbcc46c 1 1 1 15h replicaset.apps/harbor-registry-d7c5dbf6f 1 1 1 148m NAME READY AGE statefulset.apps/harbor-database 1/1 15h statefulset.apps/harbor-redis 1/1 15h statefulset.apps/harbor-trivy 1/1 15h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-24fc670f-e4f2-435d-8205-55e8681cdaa2 5Gi RWO Delete Bound harbor/data-harbor-trivy-0 nfs-client 15h persistentvolume/pvc-54c51073-e463-4337-bce7-4bf218e58e1d 1Gi RWO Delete Bound harbor/harbor-jobservice nfs-client 15h persistentvolume/pvc-9b0f92ee-439f-4e5b-bb4c-46c51c6a6317 1Gi RWO Delete Bound harbor/data-harbor-redis-0 nfs-client 15h persistentvolume/pvc-c3926a2b-eecb-4758-aabf-3d32aeb0dd52 1Gi RWO Delete Bound harbor/database-data-harbor-database-0 nfs-client 15h persistentvolume/pvc-eee40456-6920-472a-827c-b9d58179e092 5Gi RWO Delete Bound harbor/harbor-registry nfs-client 15h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/data-harbor-redis-0 Bound pvc-9b0f92ee-439f-4e5b-bb4c-46c51c6a6317 1Gi RWO nfs-client 15h persistentvolumeclaim/data-harbor-trivy-0 Bound pvc-24fc670f-e4f2-435d-8205-55e8681cdaa2 5Gi RWO nfs-client 15h persistentvolumeclaim/database-data-harbor-database-0 Bound pvc-c3926a2b-eecb-4758-aabf-3d32aeb0dd52 1Gi RWO nfs-client 15h persistentvolumeclaim/harbor-jobservice Bound pvc-54c51073-e463-4337-bce7-4bf218e58e1d 1Gi RWO nfs-client 15h persistentvolumeclaim/harbor-registry Bound pvc-eee40456-6920-472a-827c-b9d58179e092 5Gi RWO nfs-client 15h 노드 포트를 사용했기 때문에 [쿠버네티스 노드 ip]:30003 경로를 https를 이용해 들어가보면 다음과 같이 대시보드 화면이 나타나는 것을 확인하실 수 있게 됩니다. 다만, 인증서가 신뢰할 수 없다고는 뜹니다.\n저는 Envoy proxy 서버를 두었기 때문에 제가 가진 도메인과 잘 엮어서 외부에서도 접속 할 수 있게 설정을 마쳤습니다. 하버의 초기 로그인 정보는 ID : admin, PW : Harbor12345 입니다.\nHow to use? 하버를 설치 해보았으니, 사용법도 익혀봅시다. 공식 문서에서는 Working with project 부분 입니다. 눈치 빠르신 분들은 Harbor administration 파트를 넘어 갔다는 걸 아셨을 텐데, 왠지 양이 많이 뛰어 넘고 싶은 느낌 입니다. 그래도 언젠가 한 번 심도 있게 어떤 기능이 있는지 살펴보긴 해야겠죠? 언젠간 말이에요\n로그인을 하고 나면 다음과 같은 화면이 나타납니다.\nProject\n프로젝트가 생성되지 않으면 이미지를 하버에 저장할 수 없습니다. 프로젝트는 유저의 역할에 따라 사용할 수 있는 기능이 다르다고 하네요. (Role-Based Access Control : RBAC) 또 크게 Public과 Private로 구분된다고 합니다. 디폴트로 library 프로젝트가 만들어져 있습니다.\n저는 지금 관리자의 권한을 가지고 있습니다. 프로젝트를 만들어봅시다. 프로젝트 네임과 quota limit이 필수 기입 조건인데, -1이 의미하는 것은 용량 제한을 두지 않겠다 라는 말입니다.\n위의 상태로 생성을 해보면, 프로젝트가 추가된 게 확인 됩니다. Private로 만들었습니다.\n안에 들어와 보니 summary, repositories, helm charts, members, labels, scanner, p2p preheat, policy, robot accounts, logs 그리고 configuration 탭이 존재 합니다. 기능이 많습니다.\nConfiguration 쪽을 클릭해보면, Public 전환 버튼과 취약점이 있을지도 모르는 이미지(Vulnerability image)에 관련된 버튼들이 존재 합니다.\n이제 프로젝트에 유저를 추가해봅시다. 유저를 먼저 만들어 봐야겠죠? Administration에 NEW USER를 클릭합니다. 저는 openstack 이라는 이름으로 하나 만들어 보았습니다. 생성 후 project로 돌아와 Members에서 openstack을 추가 해줍시다. 선택 버튼이 아니라 입력인 것이 조금 이외입니다.\n추후 Role을 바꾼다던가 유저를 프로젝트에서 없애려면 Action 버튼을 활용하면 될 것 같습니다.\nopenstack 유저로 로그인 해서 확인해보면, kaps 프로젝트가 리스트에 보이게 됩니다.\nLDAP(?) server을 하버에 연결 시켜 두었다면, 유저에 Group을 형성할 수 있는 듯 한데, 제 하버에는 그런 설정이 되어 있지 않습니다.\n이외에도 프로젝트에서는, Robot 계정을 만들수도 있고, 웹훅 알람을 보낼 수도 있습니다. 자세한 내용은 문서를 참고해봅시다.\nImage 하버를 HTTP로 사용하거나, 인증서가 unknown CA certificate 일 때 도커 클라이언트에서 이미지를 보낼 수 없다고 합니다. 현재 제 서버가 어느 정도까지 허용 될지 모르겠으니, 우선 image push를 시도해봅시다.\n$ sudo docker image list REPOSITORY TAG IMAGE ID CREATED SIZE envoyproxy/envoy v1.27-latest 511f8ff2a1f9 5 days ago 147MB hello-world latest 9c7a54a9a43c 2 months ago 13.3kB 엔보이 프록시 이미지를 하버에 넣어봅시다. 저는 문제 없이 로그인 했습니다.\n# docker login \u0026lt;harbor_address\u0026gt; Username: admin Password: 만약 문제가 생긴다면 이 방법을 시도 해봅시다.\n다운 받아두었던 이미지를 tag 명령으로 다음과 같이 바꿔줍시다.\n# docker tag envoyproxy/envoy:v1.27-latest \u0026lt;harbor_address\u0026gt;/kaps/envoyproxy/envoy:v1.27-latest # docker image list REPOSITORY TAG IMAGE ID CREATED SIZE envoyproxy/envoy v1.27-latest 511f8ff2a1f9 5 days ago 147MB \u0026lt;harbor_address\u0026gt;/kaps/envoyproxy/envoy v1.27-latest 511f8ff2a1f9 5 days ago 147MB hello-world latest 9c7a54a9a43c 2 months ago 13.3kB 그리고 바꾼 이미지를 push 합니다.\ndocker push \u0026lt;harbor_address\u0026gt;/kaps/envoyproxy/envoy:v1.27-latest The push refers to repository [\u0026lt;harbor_address\u0026gt;/kaps/envoyproxy/envoy] 5f70bf18a086: Pushed 34049bcf227d: Pushed 7f9bdb71d8f3: Pushed d44ad2051d84: Pushed d4dab6f2fcc7: Pushed dde1e8ed09f5: Pushed c04c5616301c: Pushed 928c1abc363c: Pushed f5bb4f853c84: Pushed v1.27-latest: digest: sha256:fdc35b806570985ff51db9736017b76508873581223e5d8df92b3f8ad9c9c489 size: 2195 결과는 성공입니다!!!\n가장 근본적이고, 중요한 기능을 동작 해보았습니다.\n문서를 보면, 이미지를 다루는 다양한 기능들이 존재하니 언젠가 꼭 살펴 봐야겠습니다.\nAPI V2.0 대시보드 좌측 하단에 API 문서 링크가 있습니다. 이 역시 언젠가 마음을 다 잡고 한 번 살펴보는 것이 좋아 보입니다.\n","date":"July 31, 2022","hero":"/images/hero/harbor.png","permalink":"https://kubesy.com/posts/500.-tech_dev/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-cncf-landscape/1_harbor/","summary":"\u003ch1 id=\"harbor\"\u003eHarbor\u003c/h1\u003e\n\u003cimg src=\"/posts/images/cncf/harbor/0.png\"\n    \n    \n    \n    \n    \n        class=\"center\"\n    \n\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e참고자료\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://happycloud-lee.tistory.com/165\" target=\"_blank\" rel=\"noopener\"\u003ehttps://happycloud-lee.tistory.com/165\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://goharbor.io/docs/2.1.0/install-config/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://goharbor.io/docs/2.1.0/install-config/\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e쿠버네티스에서 파드를 생성할 때 필수적으로 필요로 하는 것이 바로 컨테이너 이미지 입니다. 컨테이너 이미지는 로컬에 저장할 수도 있고 \u003ca href=\"https://hub.docker.com\" target=\"_blank\" rel=\"noopener\"\u003e도커 허브\u003c/a\u003e와 같은 원격 환경을 이용할 수도 있습니다. 원격 환경이 다른 사람들에게 이미지를 공유하거나 할 때 편리하지만, 문제는 도커 허브 이용에 제한이 생겼다는 것 입니다. 특정 시간 동안 특정 횟수의 다운로드 제한이 있습니다.\u003c/p\u003e\n\u003cp\u003e유료 모델을 사용하는 것도 하나의 방법이지만, 그보다 무료 이미지 저장소를 로컬에 설치 하는 것이 더 바람직할 듯 합니다. Harbor는 그 목적에 아주 적합 합니다.\u003c/p\u003e","tags":null,"title":"Harbor"}]